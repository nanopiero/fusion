{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nanopiero/fusion/blob/main/notebooks/fcns/training_A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A2.radar + cmls -> pluvios1min + pluvios60min [xrl_yg1g60]"
      ],
      "metadata": {
        "id": "-1ciEeyNevrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/nanopiero/fusion.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7O7I9ZuLLvv",
        "outputId": "d2b4199d-3f60-48dc-83c7-8a5cdfc35839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fusion'...\n",
            "remote: Enumerating objects: 192, done.\u001b[K\n",
            "remote: Counting objects: 100% (185/185), done.\u001b[K\n",
            "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
            "remote: Total 192 (delta 116), reused 148 (delta 96), pack-reused 7\u001b[K\n",
            "Receiving objects: 100% (192/192), 9.78 MiB | 10.53 MiB/s, done.\n",
            "Resolving deltas: 100% (116/116), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mU0zdFYCLdgR"
      },
      "outputs": [],
      "source": [
        "# Imports des bibliothèques utiles\n",
        "# pour l'IA\n",
        "import torch\n",
        "# pour les maths\n",
        "import numpy as np\n",
        "# pour afficher des images et des courbes\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from random import randint\n",
        "import os\n",
        "import time\n",
        "\n",
        "# imports des fichiers locaux\n",
        "os.chdir('fusion')\n",
        "import utile_fusion\n",
        "# import importlib\n",
        "# importlib.reload(utile_fusion)\n",
        "\n",
        "# Import des fonctions génératrices exploitées à l'échelle de l'image\n",
        "from utile_fusion import spatialized_gt, create_cmls_filter\n",
        "# Import loading tools\n",
        "from utile_fusion import FusionDataset\n",
        "from torch.utils.data import DataLoader\n",
        "# Import des fonctions utilisées à l'échelle du batch, sur carte GPU\n",
        "from utile_fusion import indices_to_sampled_values, get_point_measurements, point_gt, segment_gt, make_noisy_images\n",
        "# Import cost functions\n",
        "from utile_fusion import QPELoss_fcn, compute_metrics\n",
        "# Import des fonctions de visualisation\n",
        "from utile_fusion import set_tensor_values2, plot_images, plot_images_10pts_20seg, plot_results_10pts_20seg\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gLGZHvSIF5NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A2. Radar + CMLS -> pluvios 1min  [xrl_yp1p60]"
      ],
      "metadata": {
        "id": "jJ_rhI2bTEbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# config de base (change en B.):\n",
        "npoints = 10\n",
        "npairs = 20\n",
        "nsteps = 60\n",
        "ndiscs = 5\n",
        "size_image=64\n",
        "length_dataset = 6400\n",
        "device = torch.device('cuda:0')"
      ],
      "metadata": {
        "id": "aFSP1Jw2jZNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Petit UNet\n",
        "from utile_fusion import UNet\n",
        "ch_in = 72\n",
        "ch_out = nsteps * 3 + 1\n",
        "size = nsteps * 3\n",
        "\n",
        "model = UNet(ch_in, ch_out, size).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
      ],
      "metadata": {
        "id": "h5xYGmWIPz3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = QPELoss_fcn(cumuls_1h=True)\n",
        "\n",
        "# Baseline with a FCN\n",
        "use_fcn = True\n",
        "\n",
        "best_loss = [float('inf'), float('inf')]  # Initialize best validation loss to a very high value\n",
        "train_losses = []"
      ],
      "metadata": {
        "id": "e1qO2JJOVuGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = r'/content/drive/MyDrive/rainCell/fusion/models/checkpoint_fcn_A2_xrl_yg1g60.pt'\n",
        "\"\"\"\n",
        "checkpoint = torch.load(path, \\\n",
        "                            map_location=device)\n",
        "last_epoch = checkpoint['epoch']\n",
        "train_losses = checkpoint['train_losses']\n",
        "# best_loss = checkpoint['best_loss']\n",
        "model_weights = checkpoint['model']\n",
        "optimizer_state_dict = checkpoint['optimizer']\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "p7vxJuh6WZvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(100):\n",
        "\n",
        "  running_regression_loss = 0.0\n",
        "  running_regression_loss_1h = 0.0\n",
        "  running_segmentation_loss = 0.0\n",
        "  train_confusion_matrix = np.zeros((2, 2), dtype=int)\n",
        "  for i, (images, pairs, filters) in enumerate(loader):\n",
        "\n",
        "    # ground truth (not usable)\n",
        "    images = images.clone().detach().float().to(device)\n",
        "\n",
        "    # pseudo CMLs\n",
        "    pairs = pairs.clone().detach().float().to(device)\n",
        "    filters = filters.clone().float().detach().to(device)\n",
        "\n",
        "    # for transformers :\n",
        "    # segment_measurements = segment_gt(images, pairs, filters)\n",
        "    _, segment_measurements_fcn = segment_gt(images, pairs, filters,\n",
        "                                             use_fcn=use_fcn)\n",
        "\n",
        "    # pseudo pluvios\n",
        "    _, point_measurements_fcn, _ = point_gt(images, npoints=npoints,\n",
        "                                            use_fcn=use_fcn)\n",
        "\n",
        "\n",
        "    # pseudo radar\n",
        "    noisy_images = make_noisy_images(images)\n",
        "\n",
        "    # prepare inputs and targets\n",
        "    inputs = torch.cat([noisy_images, segment_measurements_fcn], dim=1)\n",
        "    targets = point_measurements_fcn\n",
        "\n",
        "\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    outputs = model(inputs)  # Forward pass\n",
        "\n",
        "    regression_loss, regression_loss_1h, segmentation_loss, loss, batch_cm = criterion(model.p, outputs, targets)\n",
        "    loss.backward()  # Backward pass\n",
        "    optimizer.step()  # Update the weights\n",
        "\n",
        "    del inputs, targets, outputs, loss, noisy_images, images, pairs, filters\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    running_regression_loss += regression_loss\n",
        "    running_regression_loss_1h += regression_loss_1h\n",
        "    running_segmentation_loss += segmentation_loss\n",
        "    train_confusion_matrix += batch_cm\n",
        "\n",
        "  # Calculating average training loss\n",
        "  train_regression_loss = running_regression_loss / len(loader)\n",
        "  train_regression_loss_1h = running_regression_loss_1h / len(loader)\n",
        "  train_segmentation_loss = running_segmentation_loss / len(loader)\n",
        "  train_losses.append((epoch, train_regression_loss, train_regression_loss_1h, train_segmentation_loss, train_confusion_matrix))\n",
        "  print(f'Training, Regression Loss: {train_regression_loss:.4f}, Regression Loss 1h: {train_regression_loss_1h:.4f}, Segmentation Loss:{train_segmentation_loss:.4f}' )\n",
        "  print(\"Train Confusion Matrix:\")\n",
        "  print(train_confusion_matrix)\n",
        "  accuracy, csi, sensitivity, specificity, false_alarm_ratio = compute_metrics(train_confusion_matrix)\n",
        "  print(f'Accuracy: {accuracy:.4f}, CSI: {csi:.4f}, Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}, False Alarm Ratio: {false_alarm_ratio:.4f}')\n",
        "  print('\\n')\n"
      ],
      "metadata": {
        "id": "5iDn4YR6WKZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#100 époques : 2h28\n",
        "\n",
        "# checkpoint = {\n",
        "#     'epoch': epoch,\n",
        "#     'model': model.state_dict(),\n",
        "#     'optimizer': optimizer.state_dict(),\n",
        "#     # 'scheduler': scheduler.state_dict(),\n",
        "#     'train_losses': train_losses,\n",
        "#     }\n",
        "# torch.save(checkpoint, path)"
      ],
      "metadata": {
        "id": "LBsQkCXlB2Ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = r'/../models/checkpoint_fcn_A2_xrl_yg1g60.pt'\n",
        "checkpoint = torch.load(path, \\\n",
        "                            map_location=device)\n",
        "last_epoch = checkpoint['epoch']\n",
        "train_losses = checkpoint['train_losses']\n",
        "# best_loss = checkpoint['best_loss']\n",
        "model_weights = checkpoint['model']\n",
        "optimizer_state_dict = checkpoint['optimizer']"
      ],
      "metadata": {
        "id": "SpO1FbF0jrwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Courbe d'apprentissage\n",
        "i = -1\n",
        "j = 1 # CSI\n",
        "csi_values = [compute_metrics(x[i])[j] for x in train_losses]\n",
        "plt.plot(csi_values)"
      ],
      "metadata": {
        "id": "EBNVX0YwjlMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tracé output\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  running_regression_loss = 0.0\n",
        "  running_segmentation_loss = 0.0\n",
        "  train_confusion_matrix = np.zeros((2, 2), dtype=int)\n",
        "\n",
        "  for i, (images, pairs, filters) in enumerate(loader):\n",
        "\n",
        "    # ground truth (not usable)\n",
        "    images = images.clone().detach().float().to(device)\n",
        "\n",
        "    # pseudo CMLs\n",
        "    pairs = pairs.clone().detach().float().to(device)\n",
        "    filters = filters.clone().float().detach().to(device)\n",
        "\n",
        "    # generation point and segment measurements\n",
        "    # segment_measurements = segment_gt(images, pairs, filters)\n",
        "    segment_measurements, segment_measurements_fcn = segment_gt(images, pairs, filters, use_fcn=use_fcn)\n",
        "\n",
        "    # pseudo pluvios\n",
        "    point_measurements, point_measurements_fcn, (indices, rows, cols) = \\\n",
        "                        point_gt(images, npoints=npoints, use_fcn=use_fcn)\n",
        "\n",
        "    # pseudo radar\n",
        "    noisy_images = make_noisy_images(images)\n",
        "\n",
        "    # prepare inputs and targets\n",
        "    inputs = torch.cat([noisy_images, segment_measurements_fcn], dim=1)\n",
        "    targets = point_measurements_fcn\n",
        "    outputs = model(inputs)\n",
        "    mask_rnr = outputs[:, :nsteps,...] < outputs[:, nsteps:2*nsteps,...]\n",
        "    images_pred = (mask_rnr * outputs[:, 2*nsteps:3*nsteps, ...]).detach()\n",
        "\n",
        "    # segment_measurements = segment_gt(images, pairs, filters)\n",
        "    segment_measurements_pred, _ = segment_gt(images_pred,\n",
        "                                              pairs,\n",
        "                                              filters,\n",
        "                                              use_fcn=use_fcn)\n",
        "\n",
        "    # pseudo pluvios\n",
        "    sampled_values_pred = indices_to_sampled_values(images_pred, indices)\n",
        "    point_measurements_pred = get_point_measurements(rows, cols,\n",
        "                                                     sampled_values_pred,\n",
        "                                                     size_image)\n",
        "\n",
        "    break\n",
        "\n",
        "\n",
        "\n",
        "k=0\n",
        "\n",
        "plot_results_10pts_20seg(3*images[k,...].cpu().numpy() + filters[k,...].cpu().numpy().sum(axis=0),\n",
        "                         noisy_images[k,...].cpu().numpy(),\n",
        "                         point_measurements[k,...].cpu().numpy(),\n",
        "                         segment_measurements[k,...].cpu().numpy(),\n",
        "                         3*images_pred[k,...].cpu().numpy() + filters[k,...].cpu().numpy().sum(axis=0),\n",
        "                         (images_pred[k, torch.arange(4, 60, 5), ...] > 0).long().cpu().numpy(),\n",
        "                         point_measurements_pred[k,...].cpu().numpy(), #_pred)\n",
        "                         segment_measurements_pred[k,...].cpu().numpy()) #_pred)\n"
      ],
      "metadata": {
        "id": "bMq5OvkSj1Uz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}