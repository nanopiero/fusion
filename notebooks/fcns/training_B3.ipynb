{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/nanopiero/fusion/blob/main/notebooks/fcns/training_B11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1ciEeyNevrd"
   },
   "source": [
    "## B3 radar + gauges 1 min -> gauges 1 min [xrlg1_yg1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7O7I9ZuLLvv"
   },
   "outputs": [],
   "source": [
    "! git clone https://github.com/nanopiero/fusion.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mdso/lepetitp/ppc/WEBCAMS/src/raincell/ia/notebooks/learning/simulation/fusion/notebooks/fcns\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mU0zdFYCLdgR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "sys.path.append('/home/mdso/lepetitp/ppc/WEBCAMS/src/raincell/ia/notebooks/learning/simulation')\n",
    "\n",
    "from fusion.utils.datasets import spatialized_gt, create_cmls_filter, FusionDataset\n",
    "from fusion.utils.datasets import indices_to_sampled_values, get_point_measurements, point_gt, segment_gt, make_noisy_images\n",
    "from torch.utils.data import DataLoader\n",
    "from fusion.utils.fcn import UNet\n",
    "from fusion.utils.cost_functions import QPELoss_fcn, compute_metrics\n",
    "from fusion.utils.viz import set_tensor_values2, plot_images, plot_images_10pts_20seg, plot_results_10pts_20seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gLGZHvSIF5NW"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "P7DnFb_RWwql"
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "num_epochs = 2000\n",
    "save_every = 10\n",
    "path = r'/scratch/mdso/lepetitp/ppc/RAINCELL/models/simulation/checkpoint_fcn_exp_B3_xrg1_yg1.pt'\n",
    "npairs = 20\n",
    "nsteps = 60\n",
    "ndiscs = 5\n",
    "size_image=64\n",
    "length_dataset = 6400\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "# Entraînement\n",
    "npoints = 20\n",
    "dataset = FusionDataset(length_dataset=length_dataset,\n",
    "                        npairs=npairs,\n",
    "                        nsteps=nsteps,\n",
    "                        ndiscs=ndiscs, size_image=size_image)\n",
    "\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=64, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "seRZK-_gc-uX"
   },
   "outputs": [],
   "source": [
    "# Tiny UNet V1. 60 new channels for input time series of rain gauges measurements\n",
    "use_fcn = True\n",
    "ch_in = 72 + 60\n",
    "ch_out = nsteps * 3 + 1\n",
    "size = nsteps * 3\n",
    "\n",
    "model = UNet(ch_in, ch_out, size, nb_additional_parameters=16).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = QPELoss_fcn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "f4zwoKtaYKRl"
   },
   "outputs": [],
   "source": [
    "# Boucle avec 5 modes d'évaluation\n",
    "val_steps = ['eval_opportunity_cost_spat',\n",
    "             # 'eval_added_value_few_spat',\n",
    "             # 'eval_added_value_half_spat',\n",
    "             'eval_added_value_full_spat',\n",
    "             # 'eval_added_value_full_id',\n",
    "             ]\n",
    "steps = val_steps + ['train']\n",
    "losses = {step:[] for step in steps}\n",
    "last_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(path, \\\n",
    "                            map_location=device)\n",
    "last_epoch = checkpoint['epoch']\n",
    "losses = checkpoint['train_losses']\n",
    "# best_loss = checkpoint['best_loss']\n",
    "model_weights = checkpoint['model']\n",
    "optimizer_state_dict = checkpoint['optimizer']\n",
    "# scheduler_state_dict = checkpoint['scheduler']\n",
    "model.load_state_dict(model_weights)\n",
    "optimizer.load_state_dict(optimizer_state_dict)\n",
    "# scheduler.load_state_dict(scheduler_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1480"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEDbRb1iZG6n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch n° 1480 \n",
      "\n",
      "eval_opportunity_cost_spat, Regression Loss: 0.0716, Regression Loss 1h: 193.8541, Segmentation Loss:0.1475\n",
      "Train Confusion Matrix:\n",
      "[[3156435   43586]\n",
      " [ 125841  514138]]\n",
      "Accuracy: 0.9559, CSI: 0.7521, Sensitivity: 0.8034, Specificity: 0.9864, False Alarm Ratio: 0.0781\n",
      "\n",
      "\n",
      "eval_added_value_full_spat, Regression Loss: 0.0270, Regression Loss 1h: 237.7317, Segmentation Loss:0.0966\n",
      "Train Confusion Matrix:\n",
      "[[3147638   52383]\n",
      " [  87131  552848]]\n",
      "Accuracy: 0.9637, CSI: 0.7985, Sensitivity: 0.8639, Specificity: 0.9836, False Alarm Ratio: 0.0866\n",
      "\n",
      "\n",
      "train, Regression Loss: 0.0344, Regression Loss 1h: 229.5108, Segmentation Loss:0.1019\n",
      "Train Confusion Matrix:\n",
      "[[1743794   25112]\n",
      " [  54675  296099]]\n",
      "Accuracy: 0.9624, CSI: 0.7877, Sensitivity: 0.8441, Specificity: 0.9858, False Alarm Ratio: 0.0782\n",
      "\n",
      "\n",
      "epoch duration : 113.34474468231201\n",
      "saving step\n",
      "epoch n° 1481 \n",
      "\n",
      "eval_opportunity_cost_spat, Regression Loss: 0.0720, Regression Loss 1h: 195.0961, Segmentation Loss:0.1507\n",
      "Train Confusion Matrix:\n",
      "[[3153934   42791]\n",
      " [ 129246  514029]]\n",
      "Accuracy: 0.9552, CSI: 0.7492, Sensitivity: 0.7991, Specificity: 0.9866, False Alarm Ratio: 0.0768\n",
      "\n",
      "\n",
      "eval_added_value_full_spat, Regression Loss: 0.0270, Regression Loss 1h: 242.6424, Segmentation Loss:0.0968\n",
      "Train Confusion Matrix:\n",
      "[[3145052   51673]\n",
      " [  89380  553895]]\n",
      "Accuracy: 0.9633, CSI: 0.7970, Sensitivity: 0.8611, Specificity: 0.9838, False Alarm Ratio: 0.0853\n",
      "\n",
      "\n",
      "train, Regression Loss: 0.0355, Regression Loss 1h: 229.9420, Segmentation Loss:0.1054\n",
      "Train Confusion Matrix:\n",
      "[[1811681   26060]\n",
      " [  59921  310338]]\n",
      "Accuracy: 0.9611, CSI: 0.7831, Sensitivity: 0.8382, Specificity: 0.9858, False Alarm Ratio: 0.0775\n",
      "\n",
      "\n",
      "epoch duration : 114.67185854911804\n",
      "epoch n° 1482 \n",
      "\n",
      "eval_opportunity_cost_spat, Regression Loss: 0.0713, Regression Loss 1h: 186.3388, Segmentation Loss:0.1455\n",
      "Train Confusion Matrix:\n",
      "[[3152398   44201]\n",
      " [ 123420  519981]]\n",
      "Accuracy: 0.9563, CSI: 0.7562, Sensitivity: 0.8082, Specificity: 0.9862, False Alarm Ratio: 0.0783\n",
      "\n",
      "\n",
      "eval_added_value_full_spat, Regression Loss: 0.0272, Regression Loss 1h: 240.4666, Segmentation Loss:0.0948\n",
      "Train Confusion Matrix:\n",
      "[[3144953   51646]\n",
      " [  86057  557344]]\n",
      "Accuracy: 0.9641, CSI: 0.8019, Sensitivity: 0.8662, Specificity: 0.9838, False Alarm Ratio: 0.0848\n",
      "\n",
      "\n",
      "train, Regression Loss: 0.0369, Regression Loss 1h: 230.5604, Segmentation Loss:0.1043\n",
      "Train Confusion Matrix:\n",
      "[[1816262   25741]\n",
      " [  58593  311244]]\n",
      "Accuracy: 0.9619, CSI: 0.7868, Sensitivity: 0.8416, Specificity: 0.9860, False Alarm Ratio: 0.0764\n",
      "\n",
      "\n",
      "epoch duration : 111.60740375518799\n",
      "epoch n° 1483 \n",
      "\n",
      "eval_opportunity_cost_spat, Regression Loss: 0.0701, Regression Loss 1h: 184.4326, Segmentation Loss:0.1576\n",
      "Train Confusion Matrix:\n",
      "[[3155356   41985]\n",
      " [ 136595  506064]]\n",
      "Accuracy: 0.9535, CSI: 0.7392, Sensitivity: 0.7875, Specificity: 0.9869, False Alarm Ratio: 0.0766\n",
      "\n",
      "\n",
      "eval_added_value_full_spat, Regression Loss: 0.0265, Regression Loss 1h: 235.7267, Segmentation Loss:0.0990\n",
      "Train Confusion Matrix:\n",
      "[[3148239   49102]\n",
      " [  94586  548073]]\n",
      "Accuracy: 0.9626, CSI: 0.7923, Sensitivity: 0.8528, Specificity: 0.9846, False Alarm Ratio: 0.0822\n",
      "\n",
      "\n",
      "train, Regression Loss: 0.0327, Regression Loss 1h: 228.1842, Segmentation Loss:0.1084\n",
      "Train Confusion Matrix:\n",
      "[[1687809   24553]\n",
      " [  59528  282510]]\n",
      "Accuracy: 0.9591, CSI: 0.7706, Sensitivity: 0.8260, Specificity: 0.9857, False Alarm Ratio: 0.0800\n",
      "\n",
      "\n",
      "epoch duration : 114.4611988067627\n",
      "epoch n° 1484 \n",
      "\n",
      "eval_opportunity_cost_spat, Regression Loss: 0.0731, Regression Loss 1h: 205.6397, Segmentation Loss:0.1485\n",
      "Train Confusion Matrix:\n",
      "[[3152989   45563]\n",
      " [ 123063  518385]]\n",
      "Accuracy: 0.9561, CSI: 0.7546, Sensitivity: 0.8081, Specificity: 0.9858, False Alarm Ratio: 0.0808\n",
      "\n",
      "\n",
      "eval_added_value_full_spat, Regression Loss: 0.0273, Regression Loss 1h: 249.0268, Segmentation Loss:0.0956\n",
      "Train Confusion Matrix:\n",
      "[[3147199   51353]\n",
      " [  86994  554454]]\n",
      "Accuracy: 0.9640, CSI: 0.8003, Sensitivity: 0.8644, Specificity: 0.9839, False Alarm Ratio: 0.0848\n",
      "\n",
      "\n",
      "train, Regression Loss: 0.0337, Regression Loss 1h: 238.9027, Segmentation Loss:0.1007\n",
      "Train Confusion Matrix:\n",
      "[[1654956   24305]\n",
      " [  50644  286095]]\n",
      "Accuracy: 0.9628, CSI: 0.7924, Sensitivity: 0.8496, Specificity: 0.9855, False Alarm Ratio: 0.0783\n",
      "\n",
      "\n",
      "epoch duration : 113.55589127540588\n",
      "epoch n° 1485 \n",
      "\n",
      "eval_opportunity_cost_spat, Regression Loss: 0.0756, Regression Loss 1h: 212.1305, Segmentation Loss:0.1439\n",
      "Train Confusion Matrix:\n",
      "[[3155495   45046]\n",
      " [ 120107  519352]]\n",
      "Accuracy: 0.9570, CSI: 0.7587, Sensitivity: 0.8122, Specificity: 0.9859, False Alarm Ratio: 0.0798\n",
      "\n",
      "\n",
      "eval_added_value_full_spat, Regression Loss: 0.0277, Regression Loss 1h: 256.9198, Segmentation Loss:0.0940\n",
      "Train Confusion Matrix:\n",
      "[[3149013   51528]\n",
      " [  85687  553772]]\n",
      "Accuracy: 0.9643, CSI: 0.8014, Sensitivity: 0.8660, Specificity: 0.9839, False Alarm Ratio: 0.0851\n",
      "\n",
      "\n",
      "train, Regression Loss: 0.0376, Regression Loss 1h: 242.2709, Segmentation Loss:0.1003\n",
      "Train Confusion Matrix:\n",
      "[[1844394   26754]\n",
      " [  57036  314376]]\n",
      "Accuracy: 0.9626, CSI: 0.7896, Sensitivity: 0.8464, Specificity: 0.9857, False Alarm Ratio: 0.0784\n",
      "\n",
      "\n",
      "epoch duration : 114.40899991989136\n",
      "epoch n° 1486 \n",
      "\n",
      "eval_opportunity_cost_spat, Regression Loss: 0.0736, Regression Loss 1h: 202.5783, Segmentation Loss:0.1384\n",
      "Train Confusion Matrix:\n",
      "[[3146256   44085]\n",
      " [ 115197  534462]]\n",
      "Accuracy: 0.9585, CSI: 0.7704, Sensitivity: 0.8227, Specificity: 0.9862, False Alarm Ratio: 0.0762\n",
      "\n",
      "\n",
      "eval_added_value_full_spat, Regression Loss: 0.0269, Regression Loss 1h: 242.5797, Segmentation Loss:0.0915\n",
      "Train Confusion Matrix:\n",
      "[[3140424   49917]\n",
      " [  82641  567018]]\n",
      "Accuracy: 0.9655, CSI: 0.8105, Sensitivity: 0.8728, Specificity: 0.9844, False Alarm Ratio: 0.0809\n",
      "\n",
      "\n",
      "train, Regression Loss: 0.0350, Regression Loss 1h: 235.2521, Segmentation Loss:0.0971\n",
      "Train Confusion Matrix:\n",
      "[[1709898   24161]\n",
      " [  50747  300314]]\n",
      "Accuracy: 0.9641, CSI: 0.8004, Sensitivity: 0.8554, Specificity: 0.9861, False Alarm Ratio: 0.0745\n",
      "\n",
      "\n",
      "epoch duration : 112.60620045661926\n",
      "epoch n° 1487 \n",
      "\n",
      "eval_opportunity_cost_spat, Regression Loss: 0.0704, Regression Loss 1h: 191.9978, Segmentation Loss:0.1388\n",
      "Train Confusion Matrix:\n",
      "[[3163382   41764]\n",
      " [ 118083  516771]]\n",
      "Accuracy: 0.9584, CSI: 0.7638, Sensitivity: 0.8140, Specificity: 0.9870, False Alarm Ratio: 0.0748\n",
      "\n",
      "\n",
      "eval_added_value_full_spat, Regression Loss: 0.0271, Regression Loss 1h: 237.1918, Segmentation Loss:0.0913\n",
      "Train Confusion Matrix:\n",
      "[[3156720   48426]\n",
      " [  84102  550752]]\n",
      "Accuracy: 0.9655, CSI: 0.8060, Sensitivity: 0.8675, Specificity: 0.9849, False Alarm Ratio: 0.0808\n",
      "\n",
      "\n",
      "train, Regression Loss: 0.0357, Regression Loss 1h: 226.1780, Segmentation Loss:0.1019\n",
      "Train Confusion Matrix:\n",
      "[[1774092   25454]\n",
      " [  55384  303150]]\n",
      "Accuracy: 0.9625, CSI: 0.7895, Sensitivity: 0.8455, Specificity: 0.9859, False Alarm Ratio: 0.0775\n",
      "\n",
      "\n",
      "epoch duration : 112.13425040245056\n",
      "epoch n° 1488 \n",
      "\n",
      "eval_opportunity_cost_spat, Regression Loss: 0.0710, Regression Loss 1h: 190.0964, Segmentation Loss:0.1441\n",
      "Train Confusion Matrix:\n",
      "[[3159970   42601]\n",
      " [ 123320  514109]]\n",
      "Accuracy: 0.9568, CSI: 0.7560, Sensitivity: 0.8065, Specificity: 0.9867, False Alarm Ratio: 0.0765\n",
      "\n",
      "\n",
      "eval_added_value_full_spat, Regression Loss: 0.0274, Regression Loss 1h: 244.9566, Segmentation Loss:0.0943\n",
      "Train Confusion Matrix:\n",
      "[[3153143   49428]\n",
      " [  88146  549283]]\n",
      "Accuracy: 0.9642, CSI: 0.7997, Sensitivity: 0.8617, Specificity: 0.9846, False Alarm Ratio: 0.0826\n",
      "\n",
      "\n",
      "train, Regression Loss: 0.0348, Regression Loss 1h: 230.5472, Segmentation Loss:0.1020\n",
      "Train Confusion Matrix:\n",
      "[[1678484   23985]\n",
      " [  51824  292427]]\n",
      "Accuracy: 0.9630, CSI: 0.7941, Sensitivity: 0.8495, Specificity: 0.9859, False Alarm Ratio: 0.0758\n",
      "\n",
      "\n",
      "epoch duration : 115.16944909095764\n",
      "epoch n° 1489 \n",
      "\n",
      "eval_opportunity_cost_spat, Regression Loss: 0.0699, Regression Loss 1h: 200.5387, Segmentation Loss:0.1540\n",
      "Train Confusion Matrix:\n",
      "[[3158981   44176]\n",
      " [ 131025  505818]]\n",
      "Accuracy: 0.9544, CSI: 0.7427, Sensitivity: 0.7943, Specificity: 0.9862, False Alarm Ratio: 0.0803\n",
      "\n",
      "\n",
      "eval_added_value_full_spat, Regression Loss: 0.0270, Regression Loss 1h: 253.3639, Segmentation Loss:0.0977\n",
      "Train Confusion Matrix:\n",
      "[[3150016   53141]\n",
      " [  89343  547500]]\n",
      "Accuracy: 0.9629, CSI: 0.7935, Sensitivity: 0.8597, Specificity: 0.9834, False Alarm Ratio: 0.0885\n",
      "\n",
      "\n",
      "train, Regression Loss: 0.0344, Regression Loss 1h: 239.8486, Segmentation Loss:0.1074\n",
      "Train Confusion Matrix:\n",
      "[[1745397   25126]\n",
      " [  58721  290436]]\n",
      "Accuracy: 0.9604, CSI: 0.7760, Sensitivity: 0.8318, Specificity: 0.9858, False Alarm Ratio: 0.0796\n",
      "\n",
      "\n",
      "epoch duration : 114.61019563674927\n",
      "epoch n° 1490 \n",
      "\n",
      "eval_opportunity_cost_spat, Regression Loss: 0.0738, Regression Loss 1h: 204.6763, Segmentation Loss:0.1435\n",
      "Train Confusion Matrix:\n",
      "[[3156072   43837]\n",
      " [ 120258  519833]]\n",
      "Accuracy: 0.9573, CSI: 0.7601, Sensitivity: 0.8121, Specificity: 0.9863, False Alarm Ratio: 0.0778\n",
      "\n",
      "\n",
      "eval_added_value_full_spat, Regression Loss: 0.0267, Regression Loss 1h: 245.7475, Segmentation Loss:0.0934\n",
      "Train Confusion Matrix:\n",
      "[[3148487   51422]\n",
      " [  84658  555433]]\n",
      "Accuracy: 0.9646, CSI: 0.8032, Sensitivity: 0.8677, Specificity: 0.9839, False Alarm Ratio: 0.0847\n",
      "\n",
      "\n",
      "train, Regression Loss: 0.0331, Regression Loss 1h: 233.8290, Segmentation Loss:0.0979\n",
      "Train Confusion Matrix:\n",
      "[[1562518   23048]\n",
      " [  47444  271630]]\n",
      "Accuracy: 0.9630, CSI: 0.7940, Sensitivity: 0.8513, Specificity: 0.9855, False Alarm Ratio: 0.0782\n",
      "\n",
      "\n",
      "epoch duration : 112.77474308013916\n",
      "saving step\n",
      "epoch n° 1491 \n",
      "\n",
      "eval_opportunity_cost_spat, Regression Loss: 0.0654, Regression Loss 1h: 183.9481, Segmentation Loss:0.1526\n",
      "Train Confusion Matrix:\n",
      "[[3153849   41060]\n",
      " [ 133838  511253]]\n",
      "Accuracy: 0.9545, CSI: 0.7451, Sensitivity: 0.7925, Specificity: 0.9871, False Alarm Ratio: 0.0743\n",
      "\n",
      "\n",
      "eval_added_value_full_spat, Regression Loss: 0.0265, Regression Loss 1h: 238.9125, Segmentation Loss:0.0985\n",
      "Train Confusion Matrix:\n",
      "[[3143589   51320]\n",
      " [  90591  554500]]\n",
      "Accuracy: 0.9630, CSI: 0.7962, Sensitivity: 0.8596, Specificity: 0.9839, False Alarm Ratio: 0.0847\n",
      "\n",
      "\n",
      "train, Regression Loss: 0.0345, Regression Loss 1h: 225.8459, Segmentation Loss:0.1128\n",
      "Train Confusion Matrix:\n",
      "[[1889928   27289]\n",
      " [  67878  322745]]\n",
      "Accuracy: 0.9588, CSI: 0.7723, Sensitivity: 0.8262, Specificity: 0.9858, False Alarm Ratio: 0.0780\n",
      "\n",
      "\n",
      "epoch duration : 112.25962162017822\n",
      "epoch n° 1492 \n",
      "\n",
      "eval_opportunity_cost_spat, Regression Loss: 0.0746, Regression Loss 1h: 189.7192, Segmentation Loss:0.1434\n",
      "Train Confusion Matrix:\n",
      "[[3150024   42914]\n",
      " [ 122059  525003]]\n",
      "Accuracy: 0.9570, CSI: 0.7609, Sensitivity: 0.8114, Specificity: 0.9866, False Alarm Ratio: 0.0756\n",
      "\n",
      "\n",
      "eval_added_value_full_spat, Regression Loss: 0.0274, Regression Loss 1h: 236.1016, Segmentation Loss:0.0932\n",
      "Train Confusion Matrix:\n",
      "[[3141623   51315]\n",
      " [  85210  561852]]\n",
      "Accuracy: 0.9644, CSI: 0.8045, Sensitivity: 0.8683, Specificity: 0.9839, False Alarm Ratio: 0.0837\n",
      "\n",
      "\n",
      "train, Regression Loss: 0.0344, Regression Loss 1h: 226.9118, Segmentation Loss:0.0985\n",
      "Train Confusion Matrix:\n",
      "[[1597594   23491]\n",
      " [  48091  277704]]\n",
      "Accuracy: 0.9632, CSI: 0.7951, Sensitivity: 0.8524, Specificity: 0.9855, False Alarm Ratio: 0.0780\n",
      "\n",
      "\n",
      "epoch duration : 115.6640043258667\n",
      "epoch n° 1493 \n",
      "\n",
      "eval_opportunity_cost_spat, Regression Loss: 0.0740, Regression Loss 1h: 204.2720, Segmentation Loss:0.1426\n",
      "Train Confusion Matrix:\n",
      "[[3160083   41824]\n",
      " [ 120582  517511]]\n",
      "Accuracy: 0.9577, CSI: 0.7611, Sensitivity: 0.8110, Specificity: 0.9869, False Alarm Ratio: 0.0748\n",
      "\n",
      "\n",
      "eval_added_value_full_spat, Regression Loss: 0.0282, Regression Loss 1h: 246.8723, Segmentation Loss:0.0927\n",
      "Train Confusion Matrix:\n",
      "[[3152191   49716]\n",
      " [  85443  552650]]\n",
      "Accuracy: 0.9648, CSI: 0.8035, Sensitivity: 0.8661, Specificity: 0.9845, False Alarm Ratio: 0.0825\n",
      "\n",
      "\n",
      "train, Regression Loss: 0.0363, Regression Loss 1h: 237.2149, Segmentation Loss:0.0999\n",
      "Train Confusion Matrix:\n",
      "[[1736762   25386]\n",
      " [  53682  296170]]\n",
      "Accuracy: 0.9626, CSI: 0.7893, Sensitivity: 0.8466, Specificity: 0.9856, False Alarm Ratio: 0.0789\n",
      "\n",
      "\n",
      "epoch duration : 114.00331974029541\n",
      "epoch n° 1494 \n",
      "\n",
      "eval_opportunity_cost_spat, Regression Loss: 0.0722, Regression Loss 1h: 192.0436, Segmentation Loss:0.1389\n",
      "Train Confusion Matrix:\n",
      "[[3159300   41689]\n",
      " [ 118983  520028]]\n",
      "Accuracy: 0.9582, CSI: 0.7640, Sensitivity: 0.8138, Specificity: 0.9870, False Alarm Ratio: 0.0742\n",
      "\n",
      "\n",
      "eval_added_value_full_spat, Regression Loss: 0.0276, Regression Loss 1h: 252.8730, Segmentation Loss:0.0914\n",
      "Train Confusion Matrix:\n",
      "[[3152539   48450]\n",
      " [  85467  553544]]\n",
      "Accuracy: 0.9651, CSI: 0.8052, Sensitivity: 0.8663, Specificity: 0.9849, False Alarm Ratio: 0.0805\n",
      "\n",
      "\n",
      "train, Regression Loss: 0.0353, Regression Loss 1h: 239.2592, Segmentation Loss:0.1005\n",
      "Train Confusion Matrix:\n",
      "[[1734678   24136]\n",
      " [  54736  294610]]\n",
      "Accuracy: 0.9626, CSI: 0.7888, Sensitivity: 0.8433, Specificity: 0.9863, False Alarm Ratio: 0.0757\n",
      "\n",
      "\n",
      "epoch duration : 114.44975161552429\n",
      "epoch n° 1495 \n",
      "\n",
      "eval_opportunity_cost_spat, Regression Loss: 0.0698, Regression Loss 1h: 195.1471, Segmentation Loss:0.1562\n",
      "Train Confusion Matrix:\n",
      "[[3154211   42268]\n",
      " [ 134364  509157]]\n",
      "Accuracy: 0.9540, CSI: 0.7424, Sensitivity: 0.7912, Specificity: 0.9868, False Alarm Ratio: 0.0767\n",
      "\n",
      "\n",
      "eval_added_value_full_spat, Regression Loss: 0.0264, Regression Loss 1h: 251.7642, Segmentation Loss:0.0982\n",
      "Train Confusion Matrix:\n",
      "[[3144692   51787]\n",
      " [  90510  553011]]\n",
      "Accuracy: 0.9629, CSI: 0.7953, Sensitivity: 0.8594, Specificity: 0.9838, False Alarm Ratio: 0.0856\n",
      "\n",
      "\n",
      "train, Regression Loss: 0.0342, Regression Loss 1h: 235.4874, Segmentation Loss:0.1084\n",
      "Train Confusion Matrix:\n",
      "[[1697257   24913]\n",
      " [  58130  289460]]\n",
      "Accuracy: 0.9599, CSI: 0.7771, Sensitivity: 0.8328, Specificity: 0.9855, False Alarm Ratio: 0.0792\n",
      "\n",
      "\n",
      "epoch duration : 113.43159532546997\n",
      "epoch n° 1496 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(last_epoch, num_epochs + 1):\n",
    "  t = time.time()\n",
    "  print('epoch n°', epoch, '\\n')\n",
    "\n",
    "  running_regression_loss = {step:0.0 for step in steps}\n",
    "  running_regression_loss_1h = {step:0.0 for step in steps}\n",
    "  running_segmentation_loss = {step:0.0 for step in steps}\n",
    "  running_confusion_matrix = {step: np.zeros((2, 2), dtype=int) for step in steps}\n",
    "\n",
    "  for i, (images, pairs, filters) in enumerate(loader):\n",
    "\n",
    "    # ground truth (not usable)\n",
    "    images = images.clone().detach().float().to(device)\n",
    "\n",
    "    # pseudo radar\n",
    "    noisy_images = make_noisy_images(images)\n",
    "\n",
    "    # pseudo CMLs\n",
    "    pairs = pairs.clone().detach().float().to(device)\n",
    "    filters = filters.clone().float().detach().to(device)\n",
    "\n",
    "    # segment_measurements = segment_gt(images, pairs, filters)\n",
    "    _, segment_measurements_fcn = segment_gt(images, pairs, filters,\n",
    "                                             use_fcn=use_fcn)\n",
    "\n",
    "    #Validation steps\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "      # splitting\n",
    "      np_val_inputs_few = 1\n",
    "      np_val_inputs_half = npoints//4  # 5\n",
    "      np_val_inputs_comp = npoints//2 - np_val_inputs_half - np_val_inputs_few  # 4\n",
    "      np_val_targets = npoints//2 # 10\n",
    "\n",
    "      # val split\n",
    "      split_val = [np_val_inputs_few, np_val_inputs_half, np_val_inputs_comp, np_val_targets]\n",
    "      split_few, split_half, split_comp, split_targets = point_gt(images, npoints=npoints,\n",
    "                                                                    use_fcn=use_fcn,\n",
    "                                                                    split=split_val)\n",
    "      _, point_measurements_fcn_eval_few, _ = split_few\n",
    "      _, point_measurements_fcn_eval_half, _ = split_half\n",
    "      _, point_measurements_fcn_eval_comp, _ = split_comp\n",
    "      _, point_measurements_fcn_val_targets, _ = split_targets\n",
    "\n",
    "      # 4 first val steps (10 last pluvios for testing generalization)\n",
    "      targets = point_measurements_fcn_val_targets\n",
    "\n",
    "      # val step 1 : eval_opportunity_cost_spat\n",
    "      step = 'eval_opportunity_cost_spat'\n",
    "      inputs = torch.cat([noisy_images,\n",
    "                          0 * segment_measurements_fcn,\n",
    "                          0 * point_measurements_fcn_eval_few - 0.1\n",
    "                          ], dim=1)\n",
    "      outputs = model(inputs)\n",
    "      regression_loss, regression_loss_1h, segmentation_loss, loss, batch_cm, _ = criterion(model.p, outputs, targets)\n",
    "      running_regression_loss[step] += regression_loss\n",
    "      running_regression_loss_1h[step] += regression_loss_1h\n",
    "      running_segmentation_loss[step] += segmentation_loss\n",
    "      running_confusion_matrix[step] += batch_cm\n",
    "\n",
    "      del inputs, outputs, loss, regression_loss, regression_loss_1h, segmentation_loss\n",
    "      torch.cuda.empty_cache()\n",
    "\n",
    "      # val step 2 : eval_added_value_few_spat\n",
    "      # step = 'eval_added_value_few_spat'\n",
    "      # inputs = torch.cat([noisy_images,\n",
    "      #                     segment_measurements_fcn,\n",
    "      #                     point_measurements_fcn_eval_few\n",
    "      #                     ], dim=1)\n",
    "      # outputs = model(inputs)\n",
    "      # regression_loss, regression_loss_1h, segmentation_loss, loss, batch_cm = criterion(model.p, outputs, targets)\n",
    "      # running_regression_loss[step] += regression_loss\n",
    "      # running_regression_loss_1h[step] += regression_loss_1h\n",
    "      # running_segmentation_loss[step] += segmentation_loss\n",
    "      # running_confusion_matrix[step] += batch_cm\n",
    "\n",
    "      # del inputs, outputs, loss, regression_loss, regression_loss_1h, segmentation_loss\n",
    "      # torch.cuda.empty_cache()\n",
    "\n",
    "      # val step 3 : eval_added_value_half_spat\n",
    "      # step = 'eval_added_value_half_spat'\n",
    "      # inputs = torch.cat([noisy_images,\n",
    "      #                     segment_measurements_fcn,\n",
    "      #                     point_measurements_fcn_eval_half\n",
    "      #                     ], dim=1)\n",
    "      # outputs = model(inputs)\n",
    "      # regression_loss, regression_loss_1h, segmentation_loss, loss, batch_cm = criterion(model.p, outputs, targets)\n",
    "      # running_regression_loss[step] += regression_loss\n",
    "      # running_regression_loss_1h[step] += regression_loss_1h\n",
    "      # running_segmentation_loss[step] += segmentation_loss\n",
    "      # running_confusion_matrix[step] += batch_cm\n",
    "\n",
    "      # del inputs, outputs, loss, regression_loss, regression_loss_1h, segmentation_loss\n",
    "      # torch.cuda.empty_cache()\n",
    "\n",
    "      # val step 4 : eval_added_value_full_spat\n",
    "      step = 'eval_added_value_full_spat'\n",
    "\n",
    "      point_measurements_fcn_eval_full = point_measurements_fcn_eval_few + \\\n",
    "                                          point_measurements_fcn_eval_half + \\\n",
    "                                          point_measurements_fcn_eval_comp\n",
    "\n",
    "      point_measurements_fcn_eval_full += 2 * 0.1 * (point_measurements_fcn_eval_few >= 0)\n",
    "      point_measurements_fcn_eval_full += 2 * 0.1 * (point_measurements_fcn_eval_half >= 0)\n",
    "      point_measurements_fcn_eval_full += 2 * 0.1 * (point_measurements_fcn_eval_comp >= 0)\n",
    "\n",
    "      point_measurements_fcn_eval_full[point_measurements_fcn_eval_full<0] = -0.1\n",
    "\n",
    "      inputs = torch.cat([noisy_images,\n",
    "                          0 * segment_measurements_fcn,\n",
    "                          point_measurements_fcn_eval_full\n",
    "                          ], dim=1)\n",
    "\n",
    "      outputs = model(inputs)\n",
    "      regression_loss, regression_loss_1h, segmentation_loss, loss, batch_cm, _ = criterion(model.p, outputs, targets)\n",
    "      running_regression_loss[step] += regression_loss\n",
    "      running_regression_loss_1h[step] += regression_loss_1h\n",
    "      running_segmentation_loss[step] += segmentation_loss\n",
    "      running_confusion_matrix[step] += batch_cm\n",
    "\n",
    "      del inputs, loss, regression_loss, regression_loss_1h, segmentation_loss, split_few, split_half, split_comp, split_targets\n",
    "      torch.cuda.empty_cache()\n",
    "\n",
    "      # last val step, on the 10 first pluvios : eval_added_value_full_id\n",
    "      # step = 'eval_added_value_full_id'\n",
    "      # regression_loss, regression_loss_1h, segmentation_loss, loss, batch_cm = criterion(model.p, outputs, point_measurements_fcn_eval_full) # inputs pluvios serve as targets\n",
    "      # running_regression_loss[step] += regression_loss\n",
    "      # running_regression_loss_1h[step] += regression_loss_1h\n",
    "      # running_segmentation_loss[step] += segmentation_loss\n",
    "      # running_confusion_matrix[step] += batch_cm\n",
    "\n",
    "      # del outputs, loss, regression_loss, regression_loss_1h, segmentation_loss\n",
    "      del point_measurements_fcn_eval_full\n",
    "      torch.cuda.empty_cache()\n",
    "\n",
    "    # train step\n",
    "    model.train()\n",
    "    step = 'train'\n",
    "    np_train_inputs = torch.randint(1,9,(1,))\n",
    "    np_train_targets = npoints // 2 - np_train_inputs\n",
    "    split_train = [np_train_inputs, np_train_targets]\n",
    "\n",
    "\n",
    "    # split  train\n",
    "    split_inputs, split_targets = point_gt(images, npoints=npoints,\n",
    "                                           use_fcn=use_fcn,\n",
    "                                           split=split_train)\n",
    "\n",
    "    _, point_measurements_fcn_train_inputs, _ = split_inputs\n",
    "    _, point_measurements_fcn_train_targets, _ = split_targets\n",
    "\n",
    "    inputs0 = torch.cat([noisy_images,\n",
    "                        0 * segment_measurements_fcn,\n",
    "                        point_measurements_fcn_train_inputs\n",
    "                        ], dim=1)\n",
    "      \n",
    "    inputs1 = torch.cat([noisy_images,\n",
    "                        0 * segment_measurements_fcn,\n",
    "                        point_measurements_fcn_train_targets\n",
    "                        ], dim=1)\n",
    "      \n",
    "    targets0 = point_measurements_fcn_train_targets\n",
    "    targets1 = point_measurements_fcn_train_inputs\n",
    "      \n",
    "    optimizer.zero_grad()  # Zero the gradients\n",
    "    outputs0 = model(inputs0)\n",
    "    outputs1 = model(inputs1)      \n",
    "    regression_loss, regression_loss_1h, segmentation_loss, loss0, batch_cm, _ = criterion(model.p, outputs0, targets0)\n",
    "    _, _, _, loss1, _, _ = criterion(model.p, outputs1, targets1)\n",
    "    loss = loss0 + loss1\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update the weights\n",
    "\n",
    "    running_regression_loss[step] += regression_loss\n",
    "    running_regression_loss_1h[step] += regression_loss_1h\n",
    "    running_segmentation_loss[step] += segmentation_loss\n",
    "    running_confusion_matrix[step] += batch_cm\n",
    "\n",
    "    del split_inputs, inputs0, inputs1, outputs0, outputs1, split_targets, loss0, loss1, loss, regression_loss, regression_loss_1h, segmentation_loss, noisy_images, images, pairs, filters, segment_measurements_fcn\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "  if epoch > 0:\n",
    "    for step in steps:\n",
    "      regression_loss = running_regression_loss[step] / len(loader)\n",
    "      regression_loss_1h = running_regression_loss_1h[step] / len(loader)\n",
    "      segmentation_loss = running_segmentation_loss[step] / len(loader)\n",
    "      losses[step].append((epoch, regression_loss, regression_loss_1h, segmentation_loss, running_confusion_matrix[step]))\n",
    "\n",
    "      print(f'{step}, Regression Loss: {regression_loss:.4f}, Regression Loss 1h: {regression_loss_1h:.4f}, Segmentation Loss:{segmentation_loss:.4f}' )\n",
    "      print(\"Train Confusion Matrix:\")\n",
    "      print(running_confusion_matrix[step])\n",
    "      accuracy, csi, sensitivity, specificity, false_alarm_ratio = compute_metrics(running_confusion_matrix[step])\n",
    "      print(f'Accuracy: {accuracy:.4f}, CSI: {csi:.4f}, Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}, False Alarm Ratio: {false_alarm_ratio:.4f}')\n",
    "      print('\\n')\n",
    "  print('epoch duration :', time.time() - t)\n",
    "\n",
    "  if (epoch % save_every == 0 or \\\n",
    "    epoch == last_epoch):\n",
    "    print(\"saving step\")\n",
    "    checkpoint = { \n",
    "        'epoch': epoch,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        # 'scheduler': scheduler.state_dict(),\n",
    "        'train_losses': losses,\n",
    "        }\n",
    "    torch.save(checkpoint, path)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
