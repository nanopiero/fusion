{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nanopiero/fusion/blob/main/notebooks/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plan d'expérience:\n",
        "\n",
        "Régression\n",
        "\n",
        "      - A.baseline radar cmls -> pluvios\n",
        "\n",
        "Fusion, fusion multitâche\n",
        "\n",
        "    - B.baseline radar + pluvio 1 min + cmls -> pluvios + cmls\n",
        "\n",
        "    - C.tâches aux. 1.  A + tâche aux : -> val cml x masque segment x masque radar + val pluvio x masque pluvio x masque radar\n",
        "\n",
        "    - D. tâches aux 2. tâche reconstruction PPI.\n",
        "      - idée : f(radar[random_sample1], w) = y[70 x 64 x 64].  \n",
        "         ft de coût : ||f(radar[random_sample1])[random_sample1_bar] - target ||\n",
        "         + loss adversariale -> impossibilité de retrouver les indices renseignés\n",
        "\n",
        "    - E. tâche aux 1 + tâche aux 2. A + (B) + c\n",
        "\n",
        "Régression faiblement supervisée\n",
        "\n",
        "    - E. introduction d'un bruit dans les CMLs et les pluvios\n",
        "\n",
        "    - F. correction de l'effet du bruit dans les CMLs dans la loss (weak. sup.)\n",
        "\n",
        "    - G. correction de l'effet du bruit par un réseau auxilaire (// denoising)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-1ciEeyNevrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partie I : régression"
      ],
      "metadata": {
        "id": "JdJDsQ_lMTv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/nanopiero/fusion.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7O7I9ZuLLvv",
        "outputId": "6f8e1922-10cb-43b8-e187-52253bc74f64"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fusion'...\n",
            "remote: Enumerating objects: 101, done.\u001b[K\n",
            "remote: Counting objects: 100% (101/101), done.\u001b[K\n",
            "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
            "remote: Total 101 (delta 59), reused 4 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (101/101), 1002.68 KiB | 11.02 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mU0zdFYCLdgR"
      },
      "outputs": [],
      "source": [
        "# Imports des bibliothèques utiles\n",
        "# pour l'IA\n",
        "import torch\n",
        "# pour les maths\n",
        "import numpy as np\n",
        "# pour afficher des images et des courbes\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from random import randint\n",
        "import os\n",
        "\n",
        "# imports des fichiers locaux\n",
        "os.chdir('fusion')\n",
        "import utile_fusion\n",
        "# import importlib\n",
        "# importlib.reload(utile_fusion)\n",
        "\n",
        "# Import des fonctions génératrices exploitées à l'échelle de l'image\n",
        "from utile_fusion import spatialized_gt, create_cmls_filter\n",
        "# Import des fonctions utilisées à l'échelle du batch, sur carte GPU\n",
        "from utile_fusion import point_gt, segment_gt, make_noisy_images\n",
        "#Import des fonctions de visualisation\n",
        "from utile_fusion import set_tensor_values2, plot_images\n",
        "from utile_fusion import FusionDataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# config:\n",
        "npoints = 5\n",
        "npairs = 10\n",
        "nsteps = 60\n",
        "ndiscs = 5\n",
        "size_image=64\n",
        "length_dataset = 6400\n",
        "device = torch.device('cuda:0')"
      ],
      "metadata": {
        "id": "pVHvI49CbVb8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset, DataLoader\n",
        "\n",
        "dataset = FusionDataset(length_dataset=length_dataset, npairs=npairs, nsteps=60,\n",
        "                        ndiscs=ndiscs, size_image=size_image)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "loader = DataLoader(dataset, batch_size=64, num_workers=4)"
      ],
      "metadata": {
        "id": "NNNUcnoEMuSN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Essai avec un FCN (encodage simple)\n",
        "\n",
        "\n",
        "\n",
        "################################   UNet (parties)###############################\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class double_conv(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2'''\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(double_conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class inconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(inconv, self).__init__()\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(Down, self).__init__()\n",
        "        self.mpconv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            double_conv(in_ch, out_ch)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mpconv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=False):\n",
        "        super(Up, self).__init__()\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_ch, in_ch, kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv = double_conv(2*in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diffX = x1.size()[2] - x2.size()[2]\n",
        "        diffY = x1.size()[3] - x2.size()[3]\n",
        "        x2 = F.pad(x2, (diffX // 2, int(diffX / 2),\n",
        "                        diffY // 2, int(diffY / 2)))\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class outconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(outconv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "################################################################################\n",
        "########################################   Mini Unet  ##########################\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes,size=64):\n",
        "        super(UNet, self).__init__()\n",
        "        self.inc = inconv(n_channels, size)\n",
        "        self.down1 = Down(size, 2*size)\n",
        "        self.down2 = Down(2*size, 4*size)\n",
        "        self.down3 = Down(4*size, 8*size)\n",
        "        self.down4 = Down(8*size, 8*size)\n",
        "        self.up1 = Up(8*size, 4*size)\n",
        "        self.up2 = Up(4*size, 2*size)\n",
        "        self.up3 = Up(2*size, size)\n",
        "        self.up4 = Up(size, size)\n",
        "        self.outc = outconv(size, n_classes)\n",
        "        self.outc2 = outconv(size, n_classes)\n",
        "        self.n_classes=n_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        return   x\n",
        "\n",
        "\n",
        "# Exemple d'instanciation :\n",
        "ch_in = 14\n",
        "ch_out = 1\n",
        "size = 16\n",
        "\n",
        "model = UNet(ch_in, ch_out, size)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M1andixBciV8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(images.shape)\n",
        "bs, nsteps, S, _ = images.shape\n",
        "_, nlinks, _, _ = filters.shape\n",
        "\n",
        "filters[filters == 0] = torch.nan\n",
        "filtered_images = images.unsqueeze(dim=2) * \\\n",
        "                  filters.unsqueeze(dim=1)\n",
        "# on ajoute 1. pour distinguer du cas == 0\n",
        "sampled_values = torch.nanmean(filtered_images,\\\n",
        "                  dim=(3,4)) + 1.\n",
        "\n",
        "print(sampled_values.shape)\n",
        "filters[filters != filters] = 0\n",
        "filters = filters.unsqueeze(1)\n",
        "print(filters.shape, sampled_values.view(bs, nsteps, nlinks, 1, 1).shape)\n",
        "filters = filters * sampled_values.view(bs, nsteps, nlinks, 1, 1)\n",
        "filters = filters.sum(dim=2)\n",
        "print(filters.shape)"
      ],
      "metadata": {
        "id": "GbwZZCAj7A_-",
        "outputId": "bee50167-8973-474e-da26-3cb675906fab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 60, 64, 64])\n",
            "torch.Size([64, 60, 10])\n",
            "torch.Size([64, 1, 10, 64, 64]) torch.Size([64, 60, 10, 1, 1])\n",
            "torch.Size([64, 60, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "bs, nsteps, S, _ = images.shape\n",
        "flat_images = images.view(bs, nsteps, S * S)\n",
        "# Randomly sample M indices for each image in the batch\n",
        "indices = torch.randint(0, S * S, (bs, npoints), \\\n",
        "                        device=images.device)\n",
        "\n",
        "# Gather the values from these indices for all images\n",
        "sampled_values = torch.gather(flat_images, 2, indices.unsqueeze(dim=1).repeat([1,nsteps,1]))\n",
        "print(sampled_values.shape)\n",
        "\n",
        "\n",
        "point_measurements = torch.zeros(images.numel()).to(device)\n",
        "# point_measurements[indices]"
      ],
      "metadata": {
        "id": "88qp78MtINGp",
        "outputId": "0c6a4981-66f0-4217-b340-a150630baa85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 60, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images.numel()"
      ],
      "metadata": {
        "id": "LX8MVF-hNi-o",
        "outputId": "caf281f3-1a0b-466d-f64c-b434727af84f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15728640"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos = torch.randint(0, S, (3, npoints, npoints), \\\n",
        "                        device=images.device)\n",
        "pos"
      ],
      "metadata": {
        "id": "jgpSx5O4KGCZ",
        "outputId": "829f7aec-1576-43c1-ca1b-0271aedc9e2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[3, 3, 1, 2, 0],\n",
              "         [2, 1, 1, 2, 0],\n",
              "         [3, 3, 3, 2, 1],\n",
              "         [3, 2, 0, 0, 0],\n",
              "         [0, 2, 2, 3, 2]],\n",
              "\n",
              "        [[0, 0, 0, 2, 3],\n",
              "         [1, 2, 0, 0, 3],\n",
              "         [0, 1, 0, 3, 2],\n",
              "         [0, 2, 1, 3, 3],\n",
              "         [2, 0, 2, 1, 1]],\n",
              "\n",
              "        [[3, 2, 3, 3, 1],\n",
              "         [3, 0, 1, 3, 1],\n",
              "         [2, 2, 3, 2, 0],\n",
              "         [3, 2, 0, 3, 3],\n",
              "         [2, 3, 1, 2, 0]]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_gt_fcn(images, pairs, filters):\n",
        "  bs, nsteps, S, _ = images.shape\n",
        "  _, nlinks, _, _ = filters.shape\n",
        "\n",
        "  filters[filters == 0] = torch.nan\n",
        "  filtered_images = images.unsqueeze(dim=2) * \\\n",
        "                    filters.unsqueeze(dim=1)\n",
        "  # on ajoute 1. pour distinguer du cas == 0\n",
        "  sampled_values = torch.nanmean(filtered_images,\\\n",
        "                    dim=(3,4)) + 1.\n",
        "\n",
        "  filters[filters != filters] = 0\n",
        "  filters = filters.unsqueeze(1)\n",
        "  filters = filters * sampled_values.view(bs, nsteps, nlinks, 1, 1)\n",
        "  filters = filters.sum(dim=2)\n",
        "  return filters\n",
        "\n",
        "\n",
        "def point_gt_fcn(images, npoints=10):\n",
        "  bs, nsteps, S, _ = images.shape\n",
        "  flat_images = images.view(bs, nsteps, S * S)\n",
        "  # Randomly sample M indices for each image in the batch\n",
        "  indices = torch.randint(0, S * S, (bs, npoints), \\\n",
        "                          device=images.device)\n",
        "\n",
        "  # Gather the values from these indices for all images\n",
        "  sampled_values = torch.gather(flat_images, 2, indices.unsqueeze(dim=1).repeat([1,nsteps,1]))\n",
        "\n",
        "  # Calculate coordinates from indices\n",
        "  rows = indices // S\n",
        "  cols = indices % S\n",
        "  print(rows)\n",
        "  point_measurements = torch.zeros(images.shape).to(device)\n",
        "  for j in range(rows.shape[0]):\n",
        "    for i in range(rows.shape[1]):\n",
        "     point_measurements[j,:,rows[j,i],cols[j,i]] = images[:,:,rows[i],cols[i]]\n",
        "  return point_measurements\n",
        "\n",
        "\n",
        "# Baseline with a FCN\n",
        "\n",
        "for i, (images, pairs, filters) in enumerate(loader):\n",
        "\n",
        "  # ground truth (not usable)\n",
        "  images = images.clone().detach().float().to(device)\n",
        "\n",
        "  # pseudo CMLs\n",
        "  pairs = pairs.clone().detach().float().to(device)\n",
        "  filters = filters.clone().float().detach().to(device)\n",
        "\n",
        "  # for transformers :\n",
        "  # segment_measurements = segment_gt(images, pairs, filters)\n",
        "\n",
        "  segment_measurements = segment_gt_fcn(images, pairs, filters)\n",
        "\n",
        "  # pseudo pluvios\n",
        "  point_measurements = point_gt_fcn(images, npoints=5)\n",
        "\n",
        "  # pseudo radar\n",
        "  noisy_images = make_noisy_images(images)\n",
        "\n",
        "  # prepare inputs and targets\n",
        "  inputs = torch.cat([noisy_images, segment_measurements], dim=1)\n",
        "  targets = point_measurements\n",
        "\n",
        "\n",
        "  optimizer.zero_grad()  # Zero the gradients\n",
        "  outputs = model(inputs)  # Forward pass\n",
        "  loss = criterion(outputs, targets)  # Compute the loss\n",
        "  loss.backward()  # Backward pass\n",
        "  optimizer.step()  # Update the weights\n",
        "  aaa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Guh_xePbatqi",
        "outputId": "42ceca7a-f9e8-4e7d-ef5c-7ed4ae9ccc91"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[56, 45,  3,  4, 61],\n",
            "        [37, 25, 26, 57, 12],\n",
            "        [38,  9, 29, 60, 36],\n",
            "        [49, 18, 22, 13, 58],\n",
            "        [49, 51, 61, 56,  0],\n",
            "        [53, 63, 55, 36, 32],\n",
            "        [61,  5, 26, 31, 34],\n",
            "        [43, 12, 34, 19, 40],\n",
            "        [14,  2, 42, 35, 16],\n",
            "        [34, 12,  6, 38, 31],\n",
            "        [49,  1,  7, 57, 63],\n",
            "        [34, 16, 58, 44, 49],\n",
            "        [60, 31, 13, 59, 14],\n",
            "        [60, 57, 22, 46, 19],\n",
            "        [29, 61,  3,  1, 41],\n",
            "        [13, 26, 54, 10, 57],\n",
            "        [ 2, 20, 34, 40, 14],\n",
            "        [50, 44, 47, 52,  3],\n",
            "        [42, 20,  6,  3, 35],\n",
            "        [ 2, 63, 28, 58, 10],\n",
            "        [ 4,  7, 20, 27,  3],\n",
            "        [26, 14, 10, 37, 41],\n",
            "        [21, 50, 18, 39, 18],\n",
            "        [29, 40, 35, 41, 22],\n",
            "        [48, 45, 28, 46, 15],\n",
            "        [61, 40, 51, 62,  7],\n",
            "        [ 5, 25, 25,  0, 48],\n",
            "        [35, 11, 12, 20, 13],\n",
            "        [19, 50,  3, 45, 36],\n",
            "        [62, 28, 17, 33, 30],\n",
            "        [56, 48, 39, 12, 32],\n",
            "        [57, 53, 13,  4, 56],\n",
            "        [58, 50,  8, 61, 24],\n",
            "        [55,  0, 38,  6, 16],\n",
            "        [18, 53, 27,  6, 16],\n",
            "        [42, 32, 27, 27,  3],\n",
            "        [31, 52,  1, 15, 27],\n",
            "        [44, 26,  8, 41, 54],\n",
            "        [ 6, 21, 21, 55,  4],\n",
            "        [53, 46, 31, 56, 18],\n",
            "        [39,  8, 47, 43, 29],\n",
            "        [19, 24, 29,  2, 35],\n",
            "        [43, 61, 53, 38,  9],\n",
            "        [26, 54, 35, 41, 17],\n",
            "        [44, 41, 44,  5, 52],\n",
            "        [43,  4, 50, 40, 37],\n",
            "        [50, 17, 22, 59,  0],\n",
            "        [54,  1, 40, 41, 52],\n",
            "        [58, 13, 23, 52, 41],\n",
            "        [ 1, 39, 54, 53, 35],\n",
            "        [ 8, 28,  8, 48,  9],\n",
            "        [17, 36, 57, 24, 52],\n",
            "        [44, 52, 55, 49, 53],\n",
            "        [36, 21, 54,  7, 62],\n",
            "        [60, 60, 23, 36, 21],\n",
            "        [12,  6,  6, 44, 18],\n",
            "        [ 3, 14, 45, 24, 49],\n",
            "        [ 0, 29, 27, 14, 47],\n",
            "        [24, 18, 17, 56,  2],\n",
            "        [ 7, 23, 49, 58, 59],\n",
            "        [45, 11, 44, 47, 18],\n",
            "        [46, 46, 53, 35, 51],\n",
            "        [18, 59, 40, 50, 43],\n",
            "        [42,  8,  6, 31, 55]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "expand(torch.cuda.FloatTensor{[64, 60, 5]}, size=[60]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (3)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-0a6a0e7ef179>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;31m# pseudo pluvios\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m   \u001b[0mpoint_measurements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoint_gt_fcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnpoints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0;31m# pseudo radar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-0a6a0e7ef179>\u001b[0m in \u001b[0;36mpoint_gt_fcn\u001b[0;34m(images, npoints)\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m      \u001b[0mpoint_measurements\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpoint_measurements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.cuda.FloatTensor{[64, 60, 5]}, size=[60]): the number of sizes provided (1) must be greater or equal to the number of dimensions in the tensor (3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemple / tracés\n",
        "\n",
        "images = set_tensor_values2(images, point_measurements)\n",
        "plot_images(images[0,...].cpu().numpy() + filters[0,...].cpu().numpy().sum(axis=0),\n",
        "            noisy_images[0,...].cpu().numpy(),\n",
        "            point_measurements[0,...].cpu().numpy(),\n",
        "            segment_measurements[0,...].cpu().numpy())"
      ],
      "metadata": {
        "id": "sOlxg7D9n_a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4iRqqwuHoAsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ESaMQ9wAoAp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LGSWJattoAmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YdcggJIhoAj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f_U4hwy-oAhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AGtZha4VoAej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dnePd4g_oAb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Archi\n",
        "\n",
        "# cas d'un \"fusion transformer 4d\"\n",
        "# Paramètres du modèle :\n",
        "image_size = [64,64]\n",
        "channels = 1\n",
        "patch_size = 4\n",
        "d_model = 120\n",
        "mlp_expansion_ratio = 4\n",
        "d_ff = mlp_expansion_ratio * d_model\n",
        "n_heads = 4\n",
        "n_layers = 12\n",
        "\n",
        "\n",
        "model = FusionTransformer2dplus(image_size, patch_size, n_layers, d_model, d_ff, n_heads, channels=1)\n"
      ],
      "metadata": {
        "id": "4E4h1PR2gpXW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install einops\n",
        "\n",
        "#Transformer 2d, time = channels\n",
        "from utile_Transformers import Block, Decoder\n",
        "import torch.nn as nn\n",
        "\n",
        "class UnifiedEmbedding2dplus(nn.Module):\n",
        "  # le temps ici est compté comme un channel\n",
        "    def __init__(self, d_model, patch_size, channels, nsteps):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.patch_size = patch_size\n",
        "        self.channels = channels\n",
        "        self.nsteps = nsteps\n",
        "        self.dim_modality = 4\n",
        "        # Positional embedding for coordinates\n",
        "        self.coord_embed = nn.Linear(2, d_model // 3)\n",
        "\n",
        "        # Modality specific embeddings\n",
        "        self.patch_modality = nn.Parameter(torch.randn(self.dim_modality))\n",
        "        self.point_modality = nn.Parameter(torch.randn(self.dim_modality))\n",
        "        self.segment_modality = nn.Parameter(torch.randn(self.dim_modality))\n",
        "\n",
        "        # Feature embedding for radar image patches\n",
        "        self.patch_feature_embed = nn.Conv2d(channels, d_model - self.dim_modality \\\n",
        "                - 2 * (d_model // 3), kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # Feature embedding for point and segment rain rates\n",
        "        self.punctual_rain_rate_embed = nn.Linear(nsteps, d_model - self.dim_modality \\\n",
        "                                                  - 2 * (d_model // 3))\n",
        "        self.integrated_rain_rate_embed = nn.Linear(nsteps, d_model - self.dim_modality \\\n",
        "                                                   - 2 * (d_model // 3))\n",
        "\n",
        "    def forward(self, image, points, segments):\n",
        "        B, C, H, W = image.shape\n",
        "        device = image.device\n",
        "        # print(\"Image shape:\", image.shape)\n",
        "\n",
        "        # Embedding patches\n",
        "        patch_embeddings = self.patch_feature_embed(image).flatten(2).transpose(1, 2)\n",
        "        # print(\"Patch embeddings shape:\", patch_embeddings.shape)\n",
        "\n",
        "        # Create grid for patches\n",
        "        grid_x, grid_y = torch.meshgrid(torch.arange(0, H, self.patch_size), torch.arange(0, W, self.patch_size), indexing='ij')\n",
        "        grid_x = grid_x.to(device)\n",
        "        grid_y = grid_y.to(device)\n",
        "        upleft = torch.stack((grid_x.flatten(), grid_y.flatten()), dim=-1).float()\n",
        "        downright = torch.stack((grid_x.flatten() + self.patch_size, grid_y.flatten() + self.patch_size), dim=-1).float()\n",
        "        # erreur chatGPT !! patch_pos_embeddings = self.coord_embed(upleft) + self.coord_embed(downright)\n",
        "        patch_pos_embeddings = torch.cat([self.coord_embed(upleft), self.coord_embed(downright)], dim=-1)\n",
        "        patch_pos_embeddings = patch_pos_embeddings.repeat(B, 1, 1)\n",
        "        # print(\"Patch positional embeddings shape:\", patch_pos_embeddings.shape)\n",
        "\n",
        "        patch_embeddings = torch.cat([patch_embeddings, patch_pos_embeddings, self.patch_modality.unsqueeze(0).expand(B, patch_embeddings.size(1), -1)], dim=-1)\n",
        "        # print(\"Final patch embeddings shape:\", patch_embeddings.shape)\n",
        "\n",
        "        # Embedding points\n",
        "        point_pos_embeddings = self.coord_embed(points[..., :2].float())\n",
        "        # print(\"Point positional embeddings shape:\", point_pos_embeddings.shape)\n",
        "\n",
        "        point_feature_embeddings = self.punctual_rain_rate_embed(points[..., 2:].float())\n",
        "        point_embeddings = torch.cat([point_feature_embeddings, point_pos_embeddings, point_pos_embeddings, self.point_modality.unsqueeze(0).expand(B, points.size(1), -1)], dim=-1)\n",
        "        # print(\"Final point embeddings shape:\", point_embeddings.shape)\n",
        "\n",
        "        # Embedding segments\n",
        "        seg_pos_embeddings0 = self.coord_embed(segments[..., :2].float())\n",
        "        seg_pos_embeddings1 = self.coord_embed(segments[..., 2:4].float())\n",
        "        segment_feature_embeddings = self.integrated_rain_rate_embed(segments[..., 4:].float())\n",
        "        segment_embeddings = torch.cat([segment_feature_embeddings, seg_pos_embeddings0, seg_pos_embeddings1, self.segment_modality.unsqueeze(0).expand(B, segments.size(1), -1)], dim=-1)\n",
        "        # print(\"Final segment embeddings shape:\", segment_embeddings.shape)\n",
        "\n",
        "        # Concatenate all embeddings\n",
        "        embeddings = torch.cat([patch_embeddings, point_embeddings, segment_embeddings], dim=1)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.LayerNorm):\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "        nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "def trunc_normal_(tensor, mean=0, std=1):\n",
        "    nn.init.trunc_normal_(tensor, mean=mean, std=std)\n",
        "\n",
        "\n",
        "class FusionTransformer2dplus(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size,\n",
        "        patch_size,\n",
        "        n_layers,\n",
        "        d_model,\n",
        "        d_ff,\n",
        "        n_heads,\n",
        "        channels=1,\n",
        "        nsteps=60\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.ue = UnifiedEmbedding2dplus(d_model, patch_size, channels, nsteps)\n",
        "        self.patch_size = patch_size\n",
        "        self.n_layers = n_layers\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [Block(d_model, n_heads, d_ff) for _ in range(n_layers)]\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.apply(init_weights)\n",
        "\n",
        "        self.decoder = Decoder(patch_size, d_model)\n",
        "\n",
        "    def forward(self, x, y, z):\n",
        "        # Embed signal\n",
        "        x = self.ue(x, y, z)  # (B, N, D)\n",
        "\n",
        "        # Process through each transformer block\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Apply final normalization\n",
        "        x = self.norm(x)\n",
        "        x = x[:,:256,:]\n",
        "\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlSZZSEwsXL1",
        "outputId": "f8ad7555-bcea-4b5b-8236-76203251d53c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "\n",
        "model(noisy_images, point_measurements, segment_measurements)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "3VpjVwiqnVPP",
        "outputId": "51549c57-adf0-4b5d-b623-2978916b92e8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [36, 1, 4, 4], expected input[1, 12, 64, 64] to have 1 channels, but got 12 channels instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-05df95c92d29>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoint_measurements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_measurements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-f09bf7985d8e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, z)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# Embed signal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, N, D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Process through each transformer block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-f09bf7985d8e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, points, segments)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Embedding patches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mpatch_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_feature_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;31m# print(\"Patch embeddings shape:\", patch_embeddings.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [36, 1, 4, 4], expected input[1, 12, 64, 64] to have 1 channels, but got 12 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pE8_d-QJvSsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemple / tracés\n",
        "images = set_tensor_values2(images, point_measurements)\n",
        "plot_images(images[0,...].cpu().numpy() + filters[0,...].cpu().numpy().sum(axis=0),\n",
        "            noisy_images[0,...].cpu().numpy(),\n",
        "            point_measurements[0,...].cpu().numpy(),\n",
        "            segment_measurements[0,...].cpu().numpy())"
      ],
      "metadata": {
        "id": "2UXxerZlPkI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CxGr8YXPn8DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TYTmlYkf-ZE",
        "outputId": "03045e5a-eede-4524-d28d-07949f70b876"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1194, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}