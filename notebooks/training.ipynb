{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nanopiero/fusion/blob/main/notebooks/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plan d'expérience:\n",
        "\n",
        "Régression\n",
        "\n",
        "      - A.baseline radar cmls -> pluvios\n",
        "\n",
        "Fusion, fusion multitâche\n",
        "\n",
        "    - B.baseline radar + pluvio 1 min + cmls -> pluvios + cmls\n",
        "\n",
        "    - C.tâches aux. 1.  A + tâche aux : -> val cml x masque segment x masque radar + val pluvio x masque pluvio x masque radar\n",
        "\n",
        "    - D. tâches aux 2. tâche reconstruction PPI.\n",
        "      - idée : f(radar[random_sample1], w) = y[70 x 64 x 64].  \n",
        "         ft de coût : ||f(radar[random_sample1])[random_sample1_bar] - target ||\n",
        "         + loss adversariale -> impossibilité de retrouver les indices renseignés\n",
        "\n",
        "    - E. tâche aux 1 + tâche aux 2. A + (B) + c\n",
        "\n",
        "Régression faiblement supervisée\n",
        "\n",
        "    - E. introduction d'un bruit dans les CMLs et les pluvios\n",
        "\n",
        "    - F. correction de l'effet du bruit dans les CMLs dans la loss (weak. sup.)\n",
        "\n",
        "    - G. correction de l'effet du bruit par un réseau auxilaire (// denoising)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-1ciEeyNevrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partie I : régression"
      ],
      "metadata": {
        "id": "JdJDsQ_lMTv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/nanopiero/fusion.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7O7I9ZuLLvv",
        "outputId": "4c03cbb8-885d-4928-a10b-deb43c908b56"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fusion'...\n",
            "remote: Enumerating objects: 131, done.\u001b[K\n",
            "remote: Counting objects: 100% (131/131), done.\u001b[K\n",
            "remote: Compressing objects: 100% (130/130), done.\u001b[K\n",
            "remote: Total 131 (delta 78), reused 4 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (131/131), 2.86 MiB | 7.32 MiB/s, done.\n",
            "Resolving deltas: 100% (78/78), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mU0zdFYCLdgR"
      },
      "outputs": [],
      "source": [
        "# Imports des bibliothèques utiles\n",
        "# pour l'IA\n",
        "import torch\n",
        "# pour les maths\n",
        "import numpy as np\n",
        "# pour afficher des images et des courbes\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from random import randint\n",
        "import os\n",
        "\n",
        "# imports des fichiers locaux\n",
        "os.chdir('fusion')\n",
        "import utile_fusion\n",
        "# import importlib\n",
        "# importlib.reload(utile_fusion)\n",
        "\n",
        "# Import des fonctions génératrices exploitées à l'échelle de l'image\n",
        "from utile_fusion import spatialized_gt, create_cmls_filter\n",
        "# Import des fonctions utilisées à l'échelle du batch, sur carte GPU\n",
        "from utile_fusion import point_gt, segment_gt, make_noisy_images\n",
        "#Import des fonctions de visualisation\n",
        "from utile_fusion import set_tensor_values2, plot_images\n",
        "from utile_fusion import FusionDataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# config:\n",
        "npoints = 10\n",
        "npairs = 20\n",
        "nsteps = 60\n",
        "ndiscs = 5\n",
        "size_image=64\n",
        "length_dataset = 6400\n",
        "device = torch.device('cuda:0')"
      ],
      "metadata": {
        "id": "pVHvI49CbVb8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset, DataLoader\n",
        "dataset = FusionDataset(length_dataset=length_dataset,\n",
        "                        npairs=npairs,\n",
        "                        nsteps=nsteps,\n",
        "                        ndiscs=ndiscs, size_image=size_image)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "loader = DataLoader(dataset, batch_size=64, num_workers=4)"
      ],
      "metadata": {
        "id": "NNNUcnoEMuSN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Petit UNet\n",
        "from utile_fusion import UNet\n",
        "ch_in = 72\n",
        "ch_out = nsteps * 3\n",
        "size = nsteps * 3\n",
        "\n",
        "model = UNet(ch_in, ch_out, size).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
      ],
      "metadata": {
        "id": "4YvGxXOXubCK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utile_fusion import QPELoss_fcn, compute_metrics\n",
        "criterion = QPELoss_fcn()\n",
        "\n",
        "# Baseline with a FCN\n",
        "use_fcn = True\n",
        "\n",
        "best_loss = [float('inf'), float('inf')]  # Initialize best validation loss to a very high value\n",
        "train_losses = []"
      ],
      "metadata": {
        "id": "CophC2EtlVi6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for epoch in range(600):\n",
        "\n",
        "  running_regression_loss = 0.0\n",
        "  running_segmentation_loss = 0.0\n",
        "  train_confusion_matrix = np.zeros((2, 2), dtype=int)\n",
        "  for i, (images, pairs, filters) in enumerate(loader):\n",
        "\n",
        "    # ground truth (not usable)\n",
        "    images = images.clone().detach().float().to(device)\n",
        "\n",
        "    # pseudo CMLs\n",
        "    pairs = pairs.clone().detach().float().to(device)\n",
        "    filters = filters.clone().float().detach().to(device)\n",
        "\n",
        "    # for transformers :\n",
        "    # segment_measurements = segment_gt(images, pairs, filters)\n",
        "    _, segment_measurements_fcn = segment_gt(images, pairs, filters,\n",
        "                                             use_fcn=use_fcn)\n",
        "\n",
        "    # pseudo pluvios\n",
        "    _, point_measurements_fcn, _ = point_gt(images, npoints=npoints,\n",
        "                                            use_fcn=use_fcn)\n",
        "\n",
        "\n",
        "    # pseudo radar\n",
        "    noisy_images = make_noisy_images(images)\n",
        "\n",
        "    # prepare inputs and targets\n",
        "    inputs = torch.cat([noisy_images, segment_measurements_fcn], dim=1)\n",
        "    targets = point_measurements_fcn\n",
        "\n",
        "\n",
        "    optimizer.zero_grad()  # Zero the gradients\n",
        "    outputs = model(inputs)  # Forward pass\n",
        "\n",
        "    regression_loss, segmentation_loss, loss, batch_cm = criterion(model.p, outputs, targets)\n",
        "    loss.backward()  # Backward pass\n",
        "    optimizer.step()  # Update the weights\n",
        "\n",
        "    del inputs, targets, outputs, loss, noisy_images, images, pairs, filters\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    running_regression_loss += regression_loss\n",
        "    running_segmentation_loss += segmentation_loss\n",
        "    train_confusion_matrix += batch_cm\n",
        "\n",
        "  # Calculating average training loss\n",
        "  train_regression_loss = running_regression_loss / len(loader)\n",
        "  train_segmentation_loss = running_segmentation_loss / len(loader)\n",
        "  train_losses.append((epoch, train_regression_loss, train_segmentation_loss, train_confusion_matrix))\n",
        "  print(f'Training, Regression Loss: {train_regression_loss:.4f}, Segmentation Loss:{train_segmentation_loss:.4f}' )\n",
        "  print(\"Train Confusion Matrix:\")\n",
        "  print(train_confusion_matrix)\n",
        "  accuracy, csi, sensitivity, specificity, false_alarm_ratio = compute_metrics(train_confusion_matrix)\n",
        "  print(f'Accuracy: {accuracy:.4f}, CSI: {csi:.4f}, Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}, False Alarm Ratio: {false_alarm_ratio:.4f}')\n",
        "  print('\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Xk3zPTlFWPsD",
        "outputId": "6220471e-264e-4bc9-b884-5c44fdb0c80b"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training, Regression Loss: 0.0427, Segmentation Loss:0.1435\n",
            "Train Confusion Matrix:\n",
            "[[3109448   82469]\n",
            " [ 132591  515492]]\n",
            "Accuracy: 0.9440, CSI: 0.7056, Sensitivity: 0.7954, Specificity: 0.9742, False Alarm Ratio: 0.1379\n",
            "\n",
            "\n",
            "Training, Regression Loss: 0.0424, Segmentation Loss:0.1438\n",
            "Train Confusion Matrix:\n",
            "[[3114691   82164]\n",
            " [ 134883  508262]]\n",
            "Accuracy: 0.9435, CSI: 0.7008, Sensitivity: 0.7903, Specificity: 0.9743, False Alarm Ratio: 0.1392\n",
            "\n",
            "\n",
            "Training, Regression Loss: 0.0410, Segmentation Loss:0.1412\n",
            "Train Confusion Matrix:\n",
            "[[3120341   80962]\n",
            " [ 131822  506875]]\n",
            "Accuracy: 0.9446, CSI: 0.7043, Sensitivity: 0.7936, Specificity: 0.9747, False Alarm Ratio: 0.1377\n",
            "\n",
            "\n",
            "Training, Regression Loss: 0.0416, Segmentation Loss:0.1411\n",
            "Train Confusion Matrix:\n",
            "[[3122977   82467]\n",
            " [ 130051  504505]]\n",
            "Accuracy: 0.9447, CSI: 0.7036, Sensitivity: 0.7951, Specificity: 0.9743, False Alarm Ratio: 0.1405\n",
            "\n",
            "\n",
            "Training, Regression Loss: 0.0426, Segmentation Loss:0.1394\n",
            "Train Confusion Matrix:\n",
            "[[3123922   80709]\n",
            " [ 129616  505753]]\n",
            "Accuracy: 0.9452, CSI: 0.7063, Sensitivity: 0.7960, Specificity: 0.9748, False Alarm Ratio: 0.1376\n",
            "\n",
            "\n",
            "Training, Regression Loss: 0.0416, Segmentation Loss:0.1375\n",
            "Train Confusion Matrix:\n",
            "[[3114191   81527]\n",
            " [ 126625  517657]]\n",
            "Accuracy: 0.9458, CSI: 0.7132, Sensitivity: 0.8035, Specificity: 0.9745, False Alarm Ratio: 0.1361\n",
            "\n",
            "\n",
            "Training, Regression Loss: 0.0431, Segmentation Loss:0.1389\n",
            "Train Confusion Matrix:\n",
            "[[3110286   81674]\n",
            " [ 127592  520448]]\n",
            "Accuracy: 0.9455, CSI: 0.7132, Sensitivity: 0.8031, Specificity: 0.9744, False Alarm Ratio: 0.1356\n",
            "\n",
            "\n",
            "Training, Regression Loss: 0.0414, Segmentation Loss:0.1400\n",
            "Train Confusion Matrix:\n",
            "[[3122208   80576]\n",
            " [ 130538  506678]]\n",
            "Accuracy: 0.9450, CSI: 0.7059, Sensitivity: 0.7951, Specificity: 0.9748, False Alarm Ratio: 0.1372\n",
            "\n",
            "\n",
            "Training, Regression Loss: 0.0418, Segmentation Loss:0.1395\n",
            "Train Confusion Matrix:\n",
            "[[3114159   81469]\n",
            " [ 129423  514949]]\n",
            "Accuracy: 0.9451, CSI: 0.7095, Sensitivity: 0.7991, Specificity: 0.9745, False Alarm Ratio: 0.1366\n",
            "\n",
            "\n",
            "Training, Regression Loss: 0.0401, Segmentation Loss:0.1403\n",
            "Train Confusion Matrix:\n",
            "[[3113766   81701]\n",
            " [ 130089  514444]]\n",
            "Accuracy: 0.9448, CSI: 0.7084, Sensitivity: 0.7982, Specificity: 0.9744, False Alarm Ratio: 0.1370\n",
            "\n",
            "\n",
            "Training, Regression Loss: 0.0416, Segmentation Loss:0.1371\n",
            "Train Confusion Matrix:\n",
            "[[3114944   81190]\n",
            " [ 126444  517422]]\n",
            "Accuracy: 0.9459, CSI: 0.7136, Sensitivity: 0.8036, Specificity: 0.9746, False Alarm Ratio: 0.1356\n",
            "\n",
            "\n",
            "Training, Regression Loss: 0.0413, Segmentation Loss:0.1373\n",
            "Train Confusion Matrix:\n",
            "[[3115154   81110]\n",
            " [ 126518  517218]]\n",
            "Accuracy: 0.9459, CSI: 0.7136, Sensitivity: 0.8035, Specificity: 0.9746, False Alarm Ratio: 0.1356\n",
            "\n",
            "\n",
            "Training, Regression Loss: 0.0401, Segmentation Loss:0.1363\n",
            "Train Confusion Matrix:\n",
            "[[3123442   80198]\n",
            " [ 126754  509606]]\n",
            "Accuracy: 0.9461, CSI: 0.7112, Sensitivity: 0.8008, Specificity: 0.9750, False Alarm Ratio: 0.1360\n",
            "\n",
            "\n",
            "Training, Regression Loss: 0.0414, Segmentation Loss:0.1380\n",
            "Train Confusion Matrix:\n",
            "[[3108715   81650]\n",
            " [ 126858  522777]]\n",
            "Accuracy: 0.9457, CSI: 0.7149, Sensitivity: 0.8047, Specificity: 0.9744, False Alarm Ratio: 0.1351\n",
            "\n",
            "\n",
            "Training, Regression Loss: 0.0411, Segmentation Loss:0.1374\n",
            "Train Confusion Matrix:\n",
            "[[3114930   81649]\n",
            " [ 127032  516389]]\n",
            "Accuracy: 0.9457, CSI: 0.7122, Sensitivity: 0.8026, Specificity: 0.9745, False Alarm Ratio: 0.1365\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-a9626c950fc1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoisy_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mrunning_regression_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mregression_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m     \"\"\"\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_emptyCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "7Cw4kwZ7BNNL",
        "outputId": "f8d66bb1-9043-454a-e13c-5efd938e0eac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = r'../drive/MyDrive/rainCell/fusion/models/checkpoint_fcn_08062024.pt'"
      ],
      "metadata": {
        "id": "vYCj0RDABU2B"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls ../drive/MyDrive/rainCell/fusion/models/"
      ],
      "metadata": {
        "id": "f0DBVFtNCoKE"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = {\n",
        "    'epoch': epoch,\n",
        "    'model': model.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "    # 'scheduler': scheduler.state_dict(),\n",
        "    'train_losses': train_losses,\n",
        "    }\n",
        "torch.save(checkpoint, path)"
      ],
      "metadata": {
        "id": "LBsQkCXlB2Ae"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = -1\n",
        "j = 1\n",
        "csi_values = [compute_metrics(x[i])[j] for x in train_losses]\n",
        "# csi_values = [\n",
        "#     0.6252, 0.6411, 0.6467, 0.6464, 0.6481, 0.6507, 0.6464, 0.6601, 0.6589, 0.6645,\n",
        "#     0.6561, 0.6637, 0.6735, 0.6653, 0.6677, 0.6637, 0.6690, 0.6669, 0.6710, 0.6642,\n",
        "#     0.6676, 0.6734, 0.6701, 0.6670, 0.6628, 0.6713, 0.6857, 0.6701, 0.6793, 0.6815,\n",
        "#     0.6812, 0.6805, 0.6801, 0.6646, 0.6781, 0.6730, 0.6820, 0.6841, 0.6864, 0.6788,\n",
        "#     0.6874, 0.6828, 0.6772, 0.6884, 0.6823, 0.6870, 0.6897, 0.6807, 0.6885, 0.6784,\n",
        "#     0.6908, 0.6717, 0.6866, 0.6778, 0.6868, 0.6941, 0.6838, 0.6813, 0.6891, 0.6762,\n",
        "#     0.6798, 0.6895, 0.6957, 0.6903, 0.6814, 0.6907, 0.6958, 0.6797, 0.6893, 0.6880,\n",
        "#     0.6882, 0.6855, 0.6916, 0.6966, 0.6882, 0.6790, 0.6903, 0.6929, 0.6918, 0.6944,\n",
        "#     0.6887, 0.7093, 0.6971, 0.7010, 0.7006, 0.7073, 0.6933\n",
        "# ]\n",
        "\n",
        "# regression_loss_values = [\n",
        "#     0.0785, 0.0750, 0.0698, 0.0684, 0.0675, 0.0650, 0.0637, 0.0633, 0.0616, 0.0595,\n",
        "#     0.0589, 0.0583, 0.0591, 0.0592, 0.0592, 0.0572, 0.0574, 0.0572, 0.0551, 0.0554,\n",
        "#     0.0542, 0.0559, 0.0527, 0.0547, 0.0528, 0.0535, 0.0539, 0.0527, 0.0544, 0.0540,\n",
        "#     0.0545, 0.0533, 0.0550, 0.0510, 0.0536, 0.0501, 0.0537, 0.0523, 0.0525, 0.0517,\n",
        "#     0.0519, 0.0504, 0.0512, 0.0520, 0.0510, 0.0513, 0.0512, 0.0506, 0.0505, 0.0490,\n",
        "#     0.0507, 0.0492, 0.0514, 0.0484, 0.0487, 0.0506, 0.0488, 0.0482, 0.0491, 0.0477,\n",
        "#     0.0493, 0.0494, 0.0485, 0.0478, 0.0488, 0.0482, 0.0494, 0.0479, 0.0473, 0.0471,\n",
        "#     0.0478, 0.0498, 0.0472, 0.0468, 0.0480, 0.0494, 0.0480, 0.0476, 0.0481, 0.0478,\n",
        "#     0.0488, 0.0482, 0.0494, 0.0479, 0.0473, 0.0471, 0.0478, 0.0498, 0.0472, 0.0468,\n",
        "#     0.0480\n",
        "# ]\n",
        "\n",
        "plt.plot(csi_values)"
      ],
      "metadata": {
        "id": "uVP8DMTg0-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemple / tracés\n",
        "\n",
        "images = set_tensor_values2(images, point_measurements)\n",
        "k=1\n",
        "plot_images(images[k,...].cpu().numpy() + filters[k,...].cpu().numpy().sum(axis=0),\n",
        "            noisy_images[k,...].cpu().numpy(),\n",
        "            point_measurements[k,...].cpu().numpy(),\n",
        "            segment_measurements[k,...].cpu().numpy())"
      ],
      "metadata": {
        "id": "sOlxg7D9n_a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tracé output\n",
        "model.eval()\n",
        "inputs = torch.cat([noisy_images, segment_measurements_fcn], dim=1)\n",
        "outputs = model(inputs)\n",
        "mask_rnr = outputs[:, :nsteps,...] < outputs[:, nsteps:2*nsteps,...]\n",
        "pred_images = (mask_rnr * outputs[:, 2*nsteps:3*nsteps, ...]).detach()\n",
        "k=1\n",
        "plot_images(pred_images[k,...].cpu().numpy() + filters[k,...].cpu().numpy().sum(axis=0),\n",
        "            noisy_images[k,...].cpu().numpy(),\n",
        "            point_measurements[k,...].cpu().numpy(),\n",
        "            segment_measurements[k,...].cpu().numpy())"
      ],
      "metadata": {
        "id": "4iRqqwuHoAsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.gridspec as gridspec\n",
        "def plot_images_10pts_20seg(images, noisy_images, point_measurements, segment_measurements):\n",
        "    # Set up the figure with GridSpec\n",
        "    fig = plt.figure(figsize=(18, 24))\n",
        "    gs = gridspec.GridSpec(12, 7, width_ratios=[1, 1, 1, 1, 1, 1, 2])  # Last column twice as wide\n",
        "\n",
        "    # Manually create axes array for uniform handling as before\n",
        "    axs = [fig.add_subplot(gs[i, j]) for i in range(12) for j in range(7)]\n",
        "\n",
        "    # Hide all primary spines and ticks\n",
        "    for ax in axs:\n",
        "        for spine in ax.spines.values():\n",
        "            spine.set_visible(False)\n",
        "        ax.tick_params(axis='both', which='both', left=False, right=False, top=False, bottom=False, labelleft=False, labelbottom=False)\n",
        "\n",
        "    # Image and noisy image plots\n",
        "    for i in range(12):\n",
        "        image_indices = [5*i, 5*i+1, 5*i+2, 5*i+3, 5*i+4, 5*i]\n",
        "        noisy_index = i\n",
        "        for j in range(6):\n",
        "            ax = axs[i*7 + j]\n",
        "            img = noisy_images[noisy_index] if j == 5 else images[image_indices[j]]\n",
        "            # img_normalized = (img - np.min(img)) / (np.max(img) - np.min(img) + 0.000001)\n",
        "            ax.imshow(img, cmap='gray', aspect='auto')\n",
        "            ax.axis('off')\n",
        "\n",
        "    # Point and Segment measurements plots\n",
        "    for row in range(12):\n",
        "        ax_main = axs[row * 7 + 6]  # Last column in each row\n",
        "        if row < 4:  # First four rows for point measurements\n",
        "            for idx in range(3) if row in [0,1,2] else range(1):\n",
        "                ax = ax_main.inset_axes([0, 1 - (idx+1)/3, 1, 1/3])\n",
        "                ax.plot(point_measurements[2:, idx + row*3], marker='.', markevery=(4, 5), markeredgewidth=2, markeredgecolor='black')\n",
        "                label = f\"Pluvio {idx+1 + row*3}\"\n",
        "                ax.set_ylim([-0.1, 1.5])\n",
        "                coord1 = f\"x={point_measurements[0, idx + row*3]:.2f}\"  # First coordinate on a new line\n",
        "                coord2 = f\"y={point_measurements[1, idx + row*3]:.2f}\"  # Second coordinate on another new line\n",
        "                full_label = f\"{label}\\n{coord1}\\n{coord2}\"  # Combine into one string with two newlines\n",
        "                ax.set_ylabel(full_label, rotation=0, labelpad=0, fontsize=6)\n",
        "                ax.yaxis.set_label_coords(0.05, 0.4)\n",
        "                ax.tick_params(axis='both', which='both', left=False, bottom=False, labelleft=False, labelbottom=False)\n",
        "                for spine in ax.spines.values():\n",
        "                    spine.set_visible(False)\n",
        "\n",
        "        elif 4 <= row < 11:  # Next 7 rows for segment measurements\n",
        "            for idx in range(3) if row in [4,5,6,7,8,9,10] else range(1):\n",
        "                actual_idx = 3 * (row - 4) + idx\n",
        "                if actual_idx < 20:  # Ensure we don't exceed the 20 graphs\n",
        "                    ax = ax_main.inset_axes([0, 1 - (idx+1)/3, 1, 1/3])\n",
        "                    ax.plot(segment_measurements[4:, actual_idx], marker='.', markevery=(4, 5), markeredgewidth=1, markeredgecolor='black')\n",
        "                    ax.set_ylim([-0.1, 1.5])\n",
        "                    label = f\"CML {actual_idx+1}\"\n",
        "                    coord_text = f\"x1={segment_measurements[0, actual_idx]:.2f}, y1={segment_measurements[1, actual_idx]:.2f}\\nx2={segment_measurements[2, actual_idx]:.2f}, y2={segment_measurements[3, actual_idx]:.2f}\"\n",
        "                    full_label = f\"{label}\\n{coord_text}\"\n",
        "                    ax.set_ylabel(full_label, rotation=0, labelpad=0, fontsize=6)\n",
        "                    ax.yaxis.set_label_coords(0.05, 0.4)\n",
        "                    ax.tick_params(axis='both', which='both', left=False, bottom=False, labelleft=False, labelbottom=False)\n",
        "                    for spine in ax.spines.values():\n",
        "                        spine.set_visible(False)\n",
        "    # plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05, hspace=0.1, wspace=0.05)  # Adjust overall spacing\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ESaMQ9wAoAp0"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_rnr = outputs[:, :nsteps,...] < outputs[:, nsteps:2*nsteps,...]\n",
        "images_pred = (mask_rnr * outputs[:, 2*nsteps:3*nsteps, ...]).detach()\n",
        "k=1\n",
        "plot_images_10pts_20seg(3*images_pred[k,...].cpu().numpy() + filters[k,...].cpu().numpy().sum(axis=0),\n",
        "            noisy_images[k,...].cpu().numpy(),\n",
        "            point_measurements[k,...].cpu().numpy(),\n",
        "            segment_measurements[k,...].cpu().numpy())"
      ],
      "metadata": {
        "id": "YdcggJIhoAj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results_10pts_20seg(images, noisy_images, point_measurements, segment_measurements, images_pred, images_pred_5min_mask, point_measurements_pred, segment_measurements_pred):\n",
        "    # Set up the figure with GridSpec\n",
        "    fig = plt.figure(figsize=(18, 48))\n",
        "    gs = gridspec.GridSpec(24, 7, width_ratios=[1, 1, 1, 1, 1, 1, 2])  # Last column twice as wide\n",
        "\n",
        "    # Manually create axes array for uniform handling as before\n",
        "    axs = [fig.add_subplot(gs[i, j]) for i in range(24) for j in range(7)]\n",
        "\n",
        "    # Hide all primary spines and ticks\n",
        "    for ax in axs:\n",
        "        for spine in ax.spines.values():\n",
        "            spine.set_visible(False)\n",
        "        ax.tick_params(axis='both', which='both', left=False, right=False, top=False, bottom=False, labelleft=False, labelbottom=False)\n",
        "\n",
        "    # Image and noisy image plots\n",
        "    for i in range(12):\n",
        "        image_indices = [5*i, 5*i+1, 5*i+2, 5*i+3, 5*i+4, 5*i]\n",
        "        noisy_index = i\n",
        "        for j in range(6):\n",
        "            ax = axs[i*14 + j]\n",
        "            img = noisy_images[noisy_index] if j == 5 else images[image_indices[j]]\n",
        "            img_normalized = (img - np.min(img)) / (np.max(img) - np.min(img) + 0.000001)\n",
        "            ax.imshow(img, cmap='gray', aspect='auto')\n",
        "            ax.axis('off')\n",
        "\n",
        "    # Outputs and masked output plots\n",
        "    for i in range(12):\n",
        "        image_indices = [5*i, 5*i+1, 5*i+2, 5*i+3, 5*i+4, 5*i]\n",
        "        noisy_index = i\n",
        "        for j in range(6):\n",
        "            ax = axs[7 + i*14 + j]\n",
        "            img = images_pred_5min_mask[noisy_index] if j == 5 else images_pred[image_indices[j]]\n",
        "            img_normalized = (img - np.min(img)) / (np.max(img) - np.min(img) + 0.000001)\n",
        "            ax.imshow(img, cmap='gray', aspect='auto')\n",
        "            ax.axis('off')\n",
        "\n",
        "    # Point and Segment measurements plots\n",
        "    for row in range(12):\n",
        "        ax_main = axs[row * 7 + 6]  # Last column in each row\n",
        "        if row < 4:  # First four rows for point measurements\n",
        "            for idx in range(3) if row in [0,1,2] else range(1):\n",
        "                ax = ax_main.inset_axes([0, 1 - (idx+1)/3, 1, 1/3])\n",
        "                ax.plot(point_measurements[2:, idx + row*3], marker='.', markevery=(4, 5), markeredgewidth=2, markeredgecolor='black')\n",
        "                ax.plot(point_measurements_pred[2:, idx + row*3], marker='.', markevery=(4, 5), markeredgewidth=2, markeredgecolor='black', color='red')\n",
        "                label = f\"Pluvio {idx+1 + row*3}\"\n",
        "                ax.set_ylim([-0.1, 1.5])\n",
        "                coord1 = f\"x={point_measurements[0, idx + row*3]:.2f}\"  # First coordinate on a new line\n",
        "                coord2 = f\"y={point_measurements[1, idx + row*3]:.2f}\"  # Second coordinate on another new line\n",
        "                full_label = f\"{label}\\n{coord1}\\n{coord2}\"  # Combine into one string with two newlines\n",
        "                ax.set_ylabel(full_label, rotation=0, labelpad=0, fontsize=6)\n",
        "                ax.yaxis.set_label_coords(0.05, 0.4)\n",
        "                ax.tick_params(axis='both', which='both', left=False, bottom=False, labelleft=False, labelbottom=False)\n",
        "                for spine in ax.spines.values():\n",
        "                    spine.set_visible(False)\n",
        "\n",
        "        elif 4 <= row < 11:  # Next 7 rows for segment measurements\n",
        "            for idx in range(3) if row in [4,5,6,7,8,9,10] else range(1):\n",
        "                actual_idx = 3 * (row - 4) + idx\n",
        "                if actual_idx < 20:  # Ensure we don't exceed the 20 graphs\n",
        "                    ax = ax_main.inset_axes([0, 1 - (idx+1)/3, 1, 1/3])\n",
        "                    ax.plot(segment_measurements[4:, actual_idx], marker='.', markevery=(4, 5), markeredgewidth=1, markeredgecolor='black')\n",
        "                    ax.plot(segment_measurements_pred[4:, actual_idx], marker='.', markevery=(4, 5), markeredgewidth=1, markeredgecolor='black',color='red')\n",
        "                    ax.set_ylim([-0.1, 1.5])\n",
        "                    label = f\"CML {actual_idx+1}\"\n",
        "                    coord_text = f\"x1={segment_measurements[0, actual_idx]:.2f}, y1={segment_measurements[1, actual_idx]:.2f}\\nx2={segment_measurements[2, actual_idx]:.2f}, y2={segment_measurements[3, actual_idx]:.2f}\"\n",
        "                    full_label = f\"{label}\\n{coord_text}\"\n",
        "                    ax.set_ylabel(full_label, rotation=0, labelpad=0, fontsize=6)\n",
        "                    ax.yaxis.set_label_coords(0.05, 0.4)\n",
        "                    ax.tick_params(axis='both', which='both', left=False, bottom=False, labelleft=False, labelbottom=False)\n",
        "                    for spine in ax.spines.values():\n",
        "                        spine.set_visible(False)\n",
        "    # plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05, hspace=0.1, wspace=0.05)  # Adjust overall spacing\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def generate_indices_rows_and_columns(images, npoints):\n",
        "  bs, nsteps, S, _ = images.shape\n",
        "  weights = torch.ones(S**2).expand(bs, -1).to(images.device)\n",
        "  indices = torch.multinomial(weights, num_samples=npoints, replacement=False) #.to(images.device)\n",
        "\n",
        "  # Calculate coordinates from indices\n",
        "  rows = indices // S\n",
        "  cols = indices % S\n",
        "\n",
        "  # Gather the values from these indices for all images\n",
        "  indices = indices.unsqueeze(dim=1).repeat([1,nsteps,1])\n",
        "  return indices, rows, cols\n",
        "\n",
        "\n",
        "\n",
        "def indices_to_sampled_values(images, indices):\n",
        "  bs, nsteps, S, _ = images.shape\n",
        "  flat_images = images.view(bs, nsteps, S * S)\n",
        "\n",
        "  # Gather the values from these indices for all images\n",
        "  sampled_values = torch.gather(flat_images, 2, indices)\n",
        "  return sampled_values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_point_measurements(rows, cols, sampled_values, S=64):\n",
        "  # Normalize coordinates to be between 0 and 1\n",
        "  ys = (1 - rows.float()/S) - 1/(2*S)\n",
        "  xs = cols.float()/S + 1/(2*S)\n",
        "\n",
        "  # Stack the normalized coordinates with the values\n",
        "  point_measurements = torch.cat((xs.unsqueeze(1),\n",
        "                      ys.unsqueeze(1),\n",
        "                      sampled_values), dim=1)\n",
        "  return point_measurements\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def point_gt(images, npoints=10, use_fcn=False):\n",
        "  bs, nsteps, S, _ = images.shape\n",
        "\n",
        "  indices, rows, cols = generate_indices_rows_and_columns(images, npoints)\n",
        "\n",
        "  sampled_values = indices_to_sampled_values(images, indices)\n",
        "  point_measurements = get_point_measurements(rows, cols, sampled_values, S)\n",
        "\n",
        "  if not use_fcn:\n",
        "    return point_measurements, None, (indices, rows, cols)\n",
        "\n",
        "  else:\n",
        "    # Difference with point_gt:\n",
        "    point_measurements_fcn = -0.1 * torch.ones(images.numel(), device=images.device)\n",
        "    indices_batch = torch.arange(bs).repeat(60)\n",
        "    # indice du premier élément de la i ème image pour le premier time step dans images.flatten()\n",
        "    idx_i000=(torch.arange(bs, device = images.device) * nsteps).view(bs,1).expand(bs,nsteps)\n",
        "    # indices du premier élément de la i ème image pour le premier time step j dans images.flatten()\n",
        "    idx_ij00=idx_i000 + torch.arange(nsteps, device = images.device).view(1,nsteps).expand(bs,nsteps)\n",
        "    # indices à conserver :\n",
        "    idx_ijkl = S**2 * idx_ij00.unsqueeze(-1) + indices\n",
        "    point_measurements_fcn[idx_ijkl.flatten()] = sampled_values.flatten()\n",
        "\n",
        "    point_measurements_fcn = point_measurements_fcn.view(bs, nsteps, S, S)\n",
        "\n",
        "    return point_measurements, point_measurements_fcn, (indices, rows, cols)"
      ],
      "metadata": {
        "id": "f_U4hwy-oAhb"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tracé output\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "  running_regression_loss = 0.0\n",
        "  running_segmentation_loss = 0.0\n",
        "  train_confusion_matrix = np.zeros((2, 2), dtype=int)\n",
        "\n",
        "  for i, (images, pairs, filters) in enumerate(loader):\n",
        "\n",
        "    # ground truth (not usable)\n",
        "    images = images.clone().detach().float().to(device)\n",
        "\n",
        "    # pseudo CMLs\n",
        "    pairs = pairs.clone().detach().float().to(device)\n",
        "    filters = filters.clone().float().detach().to(device)\n",
        "\n",
        "    # generation point and segment measurements\n",
        "    # segment_measurements = segment_gt(images, pairs, filters)\n",
        "    segment_measurements, segment_measurements_fcn = segment_gt(images, pairs, filters, use_fcn=use_fcn)\n",
        "\n",
        "    # pseudo pluvios\n",
        "    point_measurements, point_measurements_fcn, (indices, rows, cols) = \\\n",
        "                        point_gt(images, npoints=npoints, use_fcn=use_fcn)\n",
        "\n",
        "    # pseudo radar\n",
        "    noisy_images = make_noisy_images(images)\n",
        "\n",
        "    # prepare inputs and targets\n",
        "    inputs = torch.cat([noisy_images, segment_measurements_fcn], dim=1)\n",
        "    targets = point_measurements_fcn\n",
        "    outputs = model(inputs)\n",
        "    mask_rnr = outputs[:, :nsteps,...] < outputs[:, nsteps:2*nsteps,...]\n",
        "    images_pred = (mask_rnr * outputs[:, 2*nsteps:3*nsteps, ...]).detach()\n",
        "\n",
        "    # segment_measurements = segment_gt(images, pairs, filters)\n",
        "    segment_measurements_pred, _ = segment_gt(images_pred,\n",
        "                                              pairs,\n",
        "                                              filters,\n",
        "                                              use_fcn=use_fcn)\n",
        "\n",
        "    # pseudo pluvios\n",
        "    sampled_values_pred = indices_to_sampled_values(images_pred, indices)\n",
        "    point_measurements_pred = get_point_measurements(rows, cols,\n",
        "                                                     sampled_values_pred,\n",
        "                                                     size_image)\n",
        "\n",
        "    break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LGSWJattoAmz"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k=1\n",
        "# plot_images(images[k,...].cpu().numpy() + filters[k,...].cpu().numpy().sum(axis=0),\n",
        "#             noisy_images[k,...].cpu().numpy(),\n",
        "#             point_measurements[k,...].cpu().numpy(),\n",
        "#             segment_measurements[k,...].cpu().numpy())\n",
        "\n",
        "plot_results_10pts_20seg(3*images[k,...].cpu().numpy() + filters[k,...].cpu().numpy().sum(axis=0),\n",
        "                         noisy_images[k,...].cpu().numpy(),\n",
        "                         point_measurements[k,...].cpu().numpy(),\n",
        "                         segment_measurements[k,...].cpu().numpy(),\n",
        "                         3*images_pred[k,...].cpu().numpy() + filters[k,...].cpu().numpy().sum(axis=0),\n",
        "                         (images_pred[k, torch.arange(4, 60, 5), ...] > 0).long().cpu().numpy(),\n",
        "                         point_measurements_pred[k,...].cpu().numpy(), #_pred,\n",
        "                         segment_measurements_pred[k,...].cpu().numpy()) #_pred)"
      ],
      "metadata": {
        "id": "4jbQK-H-Zvx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N0LwaOuMZvvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lL-DnpLGZvr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7nRmJQB3ZvpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PARXg6M1Zvms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zmTxWcUbZvkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6fsUCDmZZvht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PxNeyH4oZvfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ib6gVcwAZvcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9G1QUnujZvZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AGtZha4VoAej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dnePd4g_oAb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Archi\n",
        "\n",
        "# cas d'un \"fusion transformer 4d\"\n",
        "# Paramètres du modèle :\n",
        "image_size = [64,64]\n",
        "channels = 1\n",
        "patch_size = 4\n",
        "d_model = 120\n",
        "mlp_expansion_ratio = 4\n",
        "d_ff = mlp_expansion_ratio * d_model\n",
        "n_heads = 4\n",
        "n_layers = 12\n",
        "\n",
        "\n",
        "model = FusionTransformer2dplus(image_size, patch_size, n_layers, d_model, d_ff, n_heads, channels=1)\n"
      ],
      "metadata": {
        "id": "4E4h1PR2gpXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install einops\n",
        "\n",
        "#Transformer 2d, time = channels\n",
        "from utile_Transformers import Block, Decoder\n",
        "import torch.nn as nn\n",
        "\n",
        "class UnifiedEmbedding2dplus(nn.Module):\n",
        "  # le temps ici est compté comme un channel\n",
        "    def __init__(self, d_model, patch_size, channels, nsteps):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.patch_size = patch_size\n",
        "        self.channels = channels\n",
        "        self.nsteps = nsteps\n",
        "        self.dim_modality = 4\n",
        "        # Positional embedding for coordinates\n",
        "        self.coord_embed = nn.Linear(2, d_model // 3)\n",
        "\n",
        "        # Modality specific embeddings\n",
        "        self.patch_modality = nn.Parameter(torch.randn(self.dim_modality))\n",
        "        self.point_modality = nn.Parameter(torch.randn(self.dim_modality))\n",
        "        self.segment_modality = nn.Parameter(torch.randn(self.dim_modality))\n",
        "\n",
        "        # Feature embedding for radar image patches\n",
        "        self.patch_feature_embed = nn.Conv2d(channels, d_model - self.dim_modality \\\n",
        "                - 2 * (d_model // 3), kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # Feature embedding for point and segment rain rates\n",
        "        self.punctual_rain_rate_embed = nn.Linear(nsteps, d_model - self.dim_modality \\\n",
        "                                                  - 2 * (d_model // 3))\n",
        "        self.integrated_rain_rate_embed = nn.Linear(nsteps, d_model - self.dim_modality \\\n",
        "                                                   - 2 * (d_model // 3))\n",
        "\n",
        "    def forward(self, image, points, segments):\n",
        "        B, C, H, W = image.shape\n",
        "        device = image.device\n",
        "        # print(\"Image shape:\", image.shape)\n",
        "\n",
        "        # Embedding patches\n",
        "        patch_embeddings = self.patch_feature_embed(image).flatten(2).transpose(1, 2)\n",
        "        # print(\"Patch embeddings shape:\", patch_embeddings.shape)\n",
        "\n",
        "        # Create grid for patches\n",
        "        grid_x, grid_y = torch.meshgrid(torch.arange(0, H, self.patch_size), torch.arange(0, W, self.patch_size), indexing='ij')\n",
        "        grid_x = grid_x.to(device)\n",
        "        grid_y = grid_y.to(device)\n",
        "        upleft = torch.stack((grid_x.flatten(), grid_y.flatten()), dim=-1).float()\n",
        "        downright = torch.stack((grid_x.flatten() + self.patch_size, grid_y.flatten() + self.patch_size), dim=-1).float()\n",
        "        # erreur chatGPT !! patch_pos_embeddings = self.coord_embed(upleft) + self.coord_embed(downright)\n",
        "        patch_pos_embeddings = torch.cat([self.coord_embed(upleft), self.coord_embed(downright)], dim=-1)\n",
        "        patch_pos_embeddings = patch_pos_embeddings.repeat(B, 1, 1)\n",
        "        # print(\"Patch positional embeddings shape:\", patch_pos_embeddings.shape)\n",
        "\n",
        "        patch_embeddings = torch.cat([patch_embeddings, patch_pos_embeddings, self.patch_modality.unsqueeze(0).expand(B, patch_embeddings.size(1), -1)], dim=-1)\n",
        "        # print(\"Final patch embeddings shape:\", patch_embeddings.shape)\n",
        "\n",
        "        # Embedding points\n",
        "        point_pos_embeddings = self.coord_embed(points[..., :2].float())\n",
        "        # print(\"Point positional embeddings shape:\", point_pos_embeddings.shape)\n",
        "\n",
        "        point_feature_embeddings = self.punctual_rain_rate_embed(points[..., 2:].float())\n",
        "        point_embeddings = torch.cat([point_feature_embeddings, point_pos_embeddings, point_pos_embeddings, self.point_modality.unsqueeze(0).expand(B, points.size(1), -1)], dim=-1)\n",
        "        # print(\"Final point embeddings shape:\", point_embeddings.shape)\n",
        "\n",
        "        # Embedding segments\n",
        "        seg_pos_embeddings0 = self.coord_embed(segments[..., :2].float())\n",
        "        seg_pos_embeddings1 = self.coord_embed(segments[..., 2:4].float())\n",
        "        segment_feature_embeddings = self.integrated_rain_rate_embed(segments[..., 4:].float())\n",
        "        segment_embeddings = torch.cat([segment_feature_embeddings, seg_pos_embeddings0, seg_pos_embeddings1, self.segment_modality.unsqueeze(0).expand(B, segments.size(1), -1)], dim=-1)\n",
        "        # print(\"Final segment embeddings shape:\", segment_embeddings.shape)\n",
        "\n",
        "        # Concatenate all embeddings\n",
        "        embeddings = torch.cat([patch_embeddings, point_embeddings, segment_embeddings], dim=1)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.LayerNorm):\n",
        "        nn.init.constant_(m.bias, 0)\n",
        "        nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "def trunc_normal_(tensor, mean=0, std=1):\n",
        "    nn.init.trunc_normal_(tensor, mean=mean, std=std)\n",
        "\n",
        "\n",
        "class FusionTransformer2dplus(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size,\n",
        "        patch_size,\n",
        "        n_layers,\n",
        "        d_model,\n",
        "        d_ff,\n",
        "        n_heads,\n",
        "        channels=1,\n",
        "        nsteps=60\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.ue = UnifiedEmbedding2dplus(d_model, patch_size, channels, nsteps)\n",
        "        self.patch_size = patch_size\n",
        "        self.n_layers = n_layers\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [Block(d_model, n_heads, d_ff) for _ in range(n_layers)]\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.apply(init_weights)\n",
        "\n",
        "        self.decoder = Decoder(patch_size, d_model)\n",
        "\n",
        "    def forward(self, x, y, z):\n",
        "        # Embed signal\n",
        "        x = self.ue(x, y, z)  # (B, N, D)\n",
        "\n",
        "        # Process through each transformer block\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Apply final normalization\n",
        "        x = self.norm(x)\n",
        "        x = x[:,:256,:]\n",
        "\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlSZZSEwsXL1",
        "outputId": "f8ad7555-bcea-4b5b-8236-76203251d53c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "\n",
        "model(noisy_images, point_measurements, segment_measurements)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "3VpjVwiqnVPP",
        "outputId": "51549c57-adf0-4b5d-b623-2978916b92e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [36, 1, 4, 4], expected input[1, 12, 64, 64] to have 1 channels, but got 12 channels instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-05df95c92d29>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoisy_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoint_measurements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_measurements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-f09bf7985d8e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, z)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# Embed signal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, N, D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Process through each transformer block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-f09bf7985d8e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, points, segments)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Embedding patches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mpatch_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_feature_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;31m# print(\"Patch embeddings shape:\", patch_embeddings.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [36, 1, 4, 4], expected input[1, 12, 64, 64] to have 1 channels, but got 12 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pE8_d-QJvSsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemple / tracés\n",
        "images = set_tensor_values2(images, point_measurements)\n",
        "plot_images(images[0,...].cpu().numpy() + filters[0,...].cpu().numpy().sum(axis=0),\n",
        "            noisy_images[0,...].cpu().numpy(),\n",
        "            point_measurements[0,...].cpu().numpy(),\n",
        "            segment_measurements[0,...].cpu().numpy())"
      ],
      "metadata": {
        "id": "2UXxerZlPkI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CxGr8YXPn8DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TYTmlYkf-ZE",
        "outputId": "03045e5a-eede-4524-d28d-07949f70b876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1194, device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}