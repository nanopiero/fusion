{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nanopiero/exam_S3/blob/master/notebooks/probleme_et_consignes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examen de Machine Learning - Session 3\n"
      ],
      "metadata": {
        "id": "-1ciEeyNevrd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mU0zdFYCLdgR"
      },
      "outputs": [],
      "source": [
        "# Imports des bibliothèques utiles\n",
        "# pour l'IA\n",
        "import torch\n",
        "# pour les maths\n",
        "import numpy as np\n",
        "# pour afficher des images et des courbes\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/nanopiero/exam_S3.git"
      ],
      "metadata": {
        "id": "5zHe5if9b8Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install einops"
      ],
      "metadata": {
        "id": "ryjHWMbzMMGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I. Découverte du problème"
      ],
      "metadata": {
        "id": "iXg4IwvHDSyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans ce problème, il va s'agir de reconstruire un champ 2D à partir de plusieurs sources d'information. Les sources d'information sont les suivantes :\n",
        "  - des mesures ponctuelles du champ 2D\n",
        "  - un prédicteur spatialisé, qui consiste en un champ 2D bruité.\n",
        "  - des mesures par tomographie obtenues le long de segments\n",
        "\n",
        "Le but est de coder et d'entamer la comparaison entre deux méthodes d'apprentissage différentes basées sur des réseaux de neurones profonds. Par souci de simplicité, nous allons travailler sur des données de synthèse générées à la volée.\n",
        "\n",
        "Ces données peuvent être visualisées grâce à la fonction *gen_image_with_pairs*."
      ],
      "metadata": {
        "id": "bDRbxFjVjckN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from exam_S3.utile_Transformers import voir_batch2D, gen_image_with_pairs, set_tensor_values\n",
        "\n",
        "batch_size = 6\n",
        "n_points = 16\n",
        "n_pairs = 16\n",
        "full_target, point_measurements, spatial_predictor, line_measurements_viz, line_measurements = gen_image_with_pairs(6, n_pairs, n_points)\n",
        "# NB : - Le code de gen_image_with_pairs est précompilé avec numba. Le premier run est donc nettement plus long que les suivants.\n",
        "#      - Le terme \"pairs\" dans *gen_images_with_pairs* fait référence aux extrémités des segments."
      ],
      "metadata": {
        "id": "pd1BiavmQnmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# exemples de champ 2D cible complets (full_target)\n",
        "# ils contiennent des disques, qu'il va s'agir de reconstruire au mieux\n",
        "fig1 = plt.figure(1, figsize=(36, 6))\n",
        "voir_batch2D(full_target, 6, fig1, k=0, min_scale=0, max_scale=1)"
      ],
      "metadata": {
        "id": "tjEfo2BprXx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pour reconstruire full_target, on s'appuira sur des triplets contenant les positions et les\n",
        "# valeurs de mesures ponctuelles (point_measurements).\n",
        "# Précisément, ces triplets (x, y, m) contiennent :\n",
        "# - les coordonnées x, y des mesures ponctuelles dans le repère (O, A, B)\n",
        "# où O correspond au coin en bas à gauche de full_target, A au coin en bas à droite\n",
        "# et B au coin en haut à gauche.\n",
        "# - m : valeur au pixel de coordonnées (x,y) de full_target\n",
        "\n",
        "# Nous avons généré batch_size x n_points triplets :\n",
        "print(point_measurements.shape)\n",
        "\n",
        "# Pour visualiser ces mesures, on peut utiliser la fonction set_tensor_values(t,point_measurements, size):\n",
        "# qui affectent aux pixels de t de coordonnées x,y les valeurs m. Par exemple:\n",
        "point_measurements_viz = set_tensor_values( - 0.1 * torch.ones((6,1,64,64)), point_measurements, 64)\n",
        "fig2 = plt.figure(2, figsize=(36, 6))\n",
        "voir_batch2D(point_measurements_viz , 6, fig2, k=0, min_scale= -0.1, max_scale=0.5)\n",
        "# NB: bien noter le format utilisé pour le tenseur t"
      ],
      "metadata": {
        "id": "K15fy_jnrmOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On s'appuiera aussi sur des prédicteurs spatialisés bruités.\n",
        "# Les rectangles figurent le bruit. Les disques contenus dans ces images\n",
        "# sont alignés avec ceux du champ 2D à reconstruire\n",
        "# mais leurs intensités sont différentes.\n",
        "\n",
        "fig3 = plt.figure(3, figsize=(36, 6))\n",
        "voir_batch2D(spatial_predictor, 6, fig3, k=0, min_scale=0, max_scale=1)"
      ],
      "metadata": {
        "id": "UInyun2vsh1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enfin, on s'appuiera sur des mesures intégrées le long de segments.\n",
        "# Les positions de ces segments et les mesures associées sont contenues\n",
        "# dans la variable line_measurements_viz.\n",
        "# Précisément, line_measurements_viz contient des quintuplets (x0, y0, x1, y1, Is) où :\n",
        "# - les coordonnées x0, y0 de la première extrémité du segment\n",
        "# - les coordonnées x1, y1 de la seconde extrémité du segement\n",
        "# - la valeur moyenne Is du champ 2D full_target le long du segment.\n",
        "\n",
        "\n",
        "# Au-dessus, nous avons généré 6 x 16 quintuplets :\n",
        "print(line_measurements.shape)\n",
        "\n",
        "\n",
        "# Le tenseur line_measurements_viz permet de visualiser ces segments :\n",
        "fig3 = plt.figure(3, figsize=(36, 6))\n",
        "voir_batch2D(line_measurements_viz, 6, fig3, k=0, min_scale=0, max_scale=1)\n",
        "\n",
        "# NB: dans line_measurements_viz, les intensités des pixels par lesquels passent\n",
        "# les segments ont été réglées sur 0.2 + Is  (sauf aux intersections)"
      ],
      "metadata": {
        "id": "nJK99JkcvUga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## II. Consignes"
      ],
      "metadata": {
        "id": "0vWPJFKbNkxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La fonction *gen_image_with_pairs* permet d'aborder plusieurs problèmes d'apprentissage par plusieurs méthodes différentes.\n",
        "Notons $Y_c$, la cible complète spatialisée, $X_p$ l'ensemble des mesures ponctuelles disponibles, $X_s$ le prédicteur spatialisé et bruité,  $X_i$ l'ensemble des mesures intégrées selon des segments et $X^{2D}_i$ leur représentation sous forme de champ 2D (c'est à dire la variable *line_measurements_viz*).\n",
        "Les problèmes de **régression** auxquels nous allons nous intéresser sont :\n",
        "\n",
        "$\\mathcal{P}_{0}$ : $X_{s}  \\rightarrow  Y_c$ \\\n",
        "$\\mathcal{P}_{1}$ : $(X_{s}, X^{2D}_i)  \\rightarrow  Y_c$\n",
        "\n",
        "$\\mathcal{P}_{2}$ : $(X_{s}, X_i)  \\rightarrow  Y_c$ \\\n",
        "$\\mathcal{P}_{3}$ : $(X_{s}, X_p, X_i)  \\rightarrow  Y_c$ \\\n",
        "$\\mathcal{P}_{4}$ : $X_i  \\rightarrow  X^{2D}_i$\n"
      ],
      "metadata": {
        "id": "fiR7Hym24P_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le travail à faire se partage en deux parties:\n",
        "  - Aborder $\\mathcal{P}_{0}$ et $\\mathcal{P}_{1}$ avec un U-Net. Montrer que le réseau peut prendre en compte les mesures intégrées le long des segments pour améliorer les performances. \\\n",
        "  La preuve de concept peut être faite avec un U-Net relativement léger (voir Annexe A. ci-dessous). Pour $\\mathcal{P}_{1}$, les entrées seront les images à deux canaux formées par concaténation de $X_{s}$ et $X^{2D}_i$.\n",
        "  Utiliser une fonction de coût simple (eg MAE), un optimizer standard.\n",
        "\n",
        "  - Aborder $\\mathcal{P}_{2}$, $\\mathcal{P}_{3}$ et $\\mathcal{P}_{4}$ avec des *visual transformers* adaptés à des données d'entrées multimodales.\n",
        "  L'annexe B, ci-dessous fournit un exemple basique d'un tel transformer adapté au problème de régression $\\mathcal{P}_{3}$. C'est un ViT qu'on a légèrement modifié :\n",
        "    - pour de la prédiction dense, la cible $Y_c$ étant spatialisée (voir les convolution transposées du module *utile_Transformers*).\n",
        "    - pour prendre en compte les mesures ponctuelles et les mesures intégrées sans les encoder dans une image (voir la classe *UnifiedEmbedding*).\n",
        "    \n",
        "  Dans cette seconde partie, il s'agit alors :\n",
        "    - d'entraîner sur quelques centaines d'époques un tel visual transformer sur\n",
        "  $\\mathcal{P}_{2}$ et $\\mathcal{P}_{3}$.\n",
        "    - de comparer les résultats avec ceux de la partie précédente aux plans quantitatif et qualitatif.\n",
        "    - d'aborder $\\mathcal{P}_{4}$ pour évaluer la capacité d'un tel Visual Transformer à passer d'un encodage des mesures intégrées sous forme de quintuplets à leur représentation 2D."
      ],
      "metadata": {
        "id": "XUin-sed4NGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le rendu consistera en un répertoire Github public avec deux à trois notebooks contenant des résultats reproductibles et des modules python annexes avec le code nécessaire. \\\n",
        "Par reproductible, j'entends que les poids après entraînement doivent être mis à  disposition, par exemple sur un site de transfert de dossier comme GrosFichier.com. Ils doivent pouvoir être chargés dans les notebooks (voir Annexe C).\n"
      ],
      "metadata": {
        "id": "jsqPHtIHjeau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autres éléments de consigne:\n",
        "\n",
        "- Toute prise d'initiative visant à faciliter l'apprentissage (optimisation des hyperparamètres, utilisation d'un *scheduler*, utilisation d'un Visual Transformer mieux adapté à une régression par pixel, etc) sera valorisée.\n",
        "- Expliquer votre démarche et n'hésitez pas à aborder d'autres problèmes de régression que ceux évoqués ci-dessus, si vous pensez que cela peut avoir un intérêt."
      ],
      "metadata": {
        "id": "SoDxB4jLjdaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conseils:\n",
        "  - Eviter de trop pousser vos apprentissages : les ressources sous colab sont limitées. Quelques centaines d'époques (avec une époque = 64 images) suffiront. Pour l'apprentissage des UNet, 100 époques suffisent largement.\n",
        "  - Ne pas hésiter à poser des questions par mail: l'énoncé est assez ouvert et pourra être ajusté en fonction des points de blocage éventuels. Contact: pierre.lepetit@meteo.fr.\n",
        "  - Il est fortement conseillé de (re)lire les articles suivants avant d'aborder le travail :  [[Ronneberger2015]](https://arxiv.org/abs/1505.04597), [[Dosovitskiy2020]](https://arxiv.org/abs/2010.11929) et, pour l'encodage multimodal, [[Jaegle2021]](https://arxiv.org/abs/2103.03206).\n",
        "  Ces lectures permettront de mieux comprendre les codes, de se donner des pistes d'amélioration et de pouvoir répondre aux questions lors de l'oral."
      ],
      "metadata": {
        "id": "bi5QMhz2wpRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Annexe A : code d'un UNet"
      ],
      "metadata": {
        "id": "Y5w49rrAPvAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################   UNet (parties)###############################\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class double_conv(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2'''\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(double_conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class inconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(inconv, self).__init__()\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(Down, self).__init__()\n",
        "        self.mpconv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            double_conv(in_ch, out_ch)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mpconv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=False):\n",
        "        super(Up, self).__init__()\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear')\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_ch, in_ch, kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv = double_conv(2*in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diffX = x1.size()[2] - x2.size()[2]\n",
        "        diffY = x1.size()[3] - x2.size()[3]\n",
        "        x2 = F.pad(x2, (diffX // 2, int(diffX / 2),\n",
        "                        diffY // 2, int(diffY / 2)))\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class outconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(outconv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "################################################################################\n",
        "########################################   Mini Unet  ##########################\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes,size=64):\n",
        "        super(UNet, self).__init__()\n",
        "        self.inc = inconv(n_channels, size)\n",
        "        self.down1 = Down(size, 2*size)\n",
        "        self.down2 = Down(2*size, 4*size)\n",
        "        self.down3 = Down(4*size, 8*size)\n",
        "        self.down4 = Down(8*size, 8*size)\n",
        "        self.up1 = Up(8*size, 4*size)\n",
        "        self.up2 = Up(4*size, 2*size)\n",
        "        self.up3 = Up(2*size, size)\n",
        "        self.up4 = Up(size, size)\n",
        "        self.outc = outconv(size, n_classes)\n",
        "        self.outc2 = outconv(size, n_classes)\n",
        "        self.n_classes=n_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        return   x"
      ],
      "metadata": {
        "id": "KjR5_M4IoVA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemple d'instanciation :\n",
        "ch_in = 1\n",
        "ch_out = 1\n",
        "size = 16\n",
        "\n",
        "fcn = UNet(ch_in, ch_out, size)"
      ],
      "metadata": {
        "id": "1qJeoxRcpHtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Annexe B : exemple d'un visual transformer adapté au problème $\\mathcal{P}_3$"
      ],
      "metadata": {
        "id": "tfwQ5jochTW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paramètres du modèle :\n",
        "image_size = [64,64]\n",
        "channels = 1\n",
        "patch_size = 4\n",
        "d_model = 120\n",
        "mlp_expansion_ratio = 4\n",
        "d_ff = mlp_expansion_ratio * d_model\n",
        "n_heads = 4\n",
        "n_layers = 12"
      ],
      "metadata": {
        "id": "AGSx5D5Xr_MR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Module interne du réseau responsable de l'encodage des variables :\n",
        "from exam_S3.utile_Transformers import UnifiedEmbedding\n",
        "ue = UnifiedEmbedding(d_model, patch_size, channels)\n",
        "_, xp, xs, _, xi = gen_image_with_pairs(6, n_pairs, n_points)\n",
        "\n",
        "embeddings = ue(xs, xp, xi)\n",
        "print(embeddings.shape)\n"
      ],
      "metadata": {
        "id": "bhL3zpLWQatB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from exam_S3.utile_Transformers import FusionTransformer\n",
        "model = FusionTransformer(image_size, patch_size, n_layers, d_model, d_ff, n_heads, channels=1)\n",
        "_, xp, xs, _, xi = gen_image_with_pairs(6, n_pairs, n_points)\n",
        "print(model(xs, xp, xi).shape)\n"
      ],
      "metadata": {
        "id": "yUrHDDikx0on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Annexe C : téléchargement des poids d'un visual transformer pré-entraîné."
      ],
      "metadata": {
        "id": "46iifY4CTclo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FusionTransformers sur 900 époques :\n",
        "# mViT_900ep.pth : entraîné sur P3\n",
        "! wget https://www.grosfichiers.com/K3aaxZcSnX4_J2FzgRR6Mkk\n",
        "! unzip K3aaxZcSnX4_J2FzgRR6Mkk\n",
        "! rm K3aaxZcSnX4_J2FzgRR6Mkk"
      ],
      "metadata": {
        "id": "Mte3Iwt0XVtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "metadata": {
        "id": "NSGCfvW4yo3B"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}